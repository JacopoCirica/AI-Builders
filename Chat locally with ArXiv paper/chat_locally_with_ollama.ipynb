{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: tavily-python in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (0.7.7)\n",
      "Requirement already satisfied: requests in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (2.32.4)\n",
      "Requirement already satisfied: pydantic in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (2.11.7)\n",
      "Requirement already satisfied: pymupdf in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (1.26.1)\n",
      "Requirement already satisfied: PyPDF2 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: openai in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (1.88.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/jacopocirica/anaconda3/lib/python3.11/site-packages (from tiktoken>=0.5.1->tavily-python) (2022.7.9)\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama tavily-python requests pydantic pymupdf PyPDF2 openai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from tavily import TavilyClient\n",
    "from typing import Dict, Callable\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull the models to run them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull model\n",
    "ollama.pull('gemma3:4b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('llava:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('qwen3:4b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To verify the LLMs that we have currently downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            ID              SIZE      MODIFIED      \n",
      "qwen3:4b        2bfd38a7daaf    2.6 GB    2 seconds ago    \n",
      "llava:latest    8dd30f6b0cb1    4.7 GB    3 seconds ago    \n",
      "gemma3:4b       a2af6cc3eb7f    3.3 GB    5 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) You can delete them with '! ollama rm <name_of_model>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hdeleted 'llama3.2-vision:latest'\n"
     ]
    }
   ],
   "source": [
    "!ollama rm llama3.2-vision:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we test the ability of Gemma to describe and read an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let‚Äôs break down this diagram illustrating Causal Language Modeling pretraining.\n",
      "\n",
      "**Overall Concept:**\n",
      "\n",
      "The image represents a simplified model used for causal language modeling. Causal language models are designed to predict the next word in a sequence, considering only the words that came before it. This is in contrast to autoregressive models which consider the entire sequence.\n",
      "\n",
      "**Components and Flow:**\n",
      "\n",
      "1.  **Input Prompt:**  The input prompt - in this case, \"is\" - is represented as a series of colored blocks (PAD - padding).  The diagram is illustrating how the model receives the input sequence.\n",
      "\n",
      "2.  **LLM (Large Language Model):** This is the core component. It's depicted as a large square grid.  This represents the layers of the neural network responsible for processing the input sequence.\n",
      "\n",
      "3.  **Hidden States:**  The circles within the LLM represent \"hidden states.\" These are the intermediate representations that the model creates at each step during the processing of the input. Each hidden state captures some aspect of the context derived from the preceding words.\n",
      "\n",
      "4.  **Language Modeling Head (Linear Layer):** The square-shaped component to the right of the LLM is the \"language modeling head.\" It's a linear layer that takes the hidden states and converts them into prediction scores. \n",
      "\n",
      "5. **Predictions:** The blue arrows representing the predictions.  This is the output of the language modeling head.\n",
      "\n",
      "6.  **Sequence Size:** At the output is \"Sequence size\".\n",
      "\n",
      "**Key Idea:**\n",
      "\n",
      "The diagram illustrates the core process: The model receives a sequence (padded with PADs), processes it through the LLM to create hidden states, and then uses these hidden states to generate predictions for the next word in the sequence.\n",
      "\n",
      "Let me know if you would like me to elaborate on any specific aspect of this diagram!"
     ]
    }
   ],
   "source": [
    "# interact with model (locally)\n",
    "stream = ollama.chat(\n",
    "    model='gemma3:4b',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Can you describe this image?\",\n",
    "        'images': ['/Users/jacopocirica/Desktop/AI Bootcamp/Screenshot 2025-06-12 at 16.33.02.png']\n",
    "    }],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we try Qwen for the function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/abs/2311.17633'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function\n",
    "\n",
    "def get_url(topic: str):\n",
    "    tavily_client = TavilyClient(api_key=\"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\")\n",
    "    response = tavily_client.search(query=f\"Tell me the main paper about this topic:{topic}\",include_domains=[\"arxiv.org/abs/\"], search_depth= \"advanced\")\n",
    "    return response['results'][0]['url']\n",
    "\n",
    "# Call the function\n",
    "\n",
    "get_url(\"Transformer Architecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen calls the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  Mamba Architecture\n",
      "Tool call:  get_url\n",
      "URL:  https://arxiv.org/abs/2502.07161\n"
     ]
    }
   ],
   "source": [
    "available_functions: Dict[str, Callable] = {\n",
    "    \"get_url\": get_url\n",
    "}\n",
    "response = ollama.chat(\n",
    "    model='qwen3:4b',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Can you tell me the main paper about this topic: Mamba Architecture?\"\n",
    "    }],\n",
    "    tools=[get_url]\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "    for tool_call in response.message.tool_calls:\n",
    "        if tool_call.function.name == \"get_url\":\n",
    "            print(\"Topic: \", tool_call.function.arguments['topic'])\n",
    "            print(\"Tool call: \", tool_call.function.name)\n",
    "            url=available_functions[tool_call.function.name](tool_call.function.arguments['topic'])\n",
    "            print(\"URL: \", url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to download the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_arxiv_pdf(arxiv_url: str, destination_folder: str, filename: str = None):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from an arXiv URL and saves it to a specified folder.\n",
    "\n",
    "    Args:\n",
    "        arxiv_url (str): Direct link to the arXiv PDF (e.g., https://arxiv.org/pdf/2311.17633).\n",
    "        destination_folder (str): Local folder path to save the file.\n",
    "        filename (str, optional): Name to save the file as (e.g., 'paper.pdf').\n",
    "                                  If None, it uses the arXiv ID as the filename.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the saved file.\n",
    "    \"\"\"\n",
    "    # Extract default filename from URL if not provided\n",
    "    if filename is None:\n",
    "        filename = arxiv_url.strip(\"/\").split(\"/\")[-1] + \".pdf\"\n",
    "\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(arxiv_url)\n",
    "        response.raise_for_status()  # Raise error for bad responses\n",
    "        with open(destination_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"‚úÖ PDF downloaded to: {destination_path}\")\n",
    "        return destination_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF downloaded to: ./PDFs/2502.07161.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./PDFs/2502.07161.pdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_arxiv_pdf(\"https://arxiv.org/pdf/2502.07161\", \"./PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert each page of PDF into an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# pdf_path: the folder of all the PDF files\n",
    "# saved_path: the path of the saved page images\n",
    "def convert_pdf_to_image(pdf_path, pdf_file, saved_path):\n",
    "\n",
    "    if not os.path.exists(saved_path):\n",
    "        os.mkdir(saved_path)\n",
    "    else:\n",
    "        files = glob.glob('saved_path/*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    try:\n",
    "        fitz.TOOLS.mupdf_warnings()  # empty the problem message container\n",
    "        doc = fitz.open(pdf_path + \"/\" + pdf_file)\n",
    "        warnings = fitz.TOOLS.mupdf_warnings()\n",
    "        if warnings:\n",
    "            print(warnings)\n",
    "            raise RuntimeError()\n",
    "\n",
    "        for page in doc:  # iterate through the pages\n",
    "            pix = page.get_pixmap()  # render page to an image\n",
    "            pix.save(saved_path + \"/\" + f\"{pdf_file[:-4]}-{page.number}.png\")  # store image as a PNG\n",
    "        return\n",
    "\n",
    "    except:\n",
    "        print(\"error when opening the pdf file {}\".format(pdf_file))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_pdf_to_image(\"./PDFs\", \"2502.07161.pdf\", \"./PDFs/images_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract the text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filepath):\n",
    "    pdf=fitz.open(filepath)\n",
    "    text=\"\".join([page.get_text() for page in pdf])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Survey on Mamba Architecture for Vision\\nApplications\\nFady Ibrahim\\nDepartment of Computer Science\\nToronto Metropolitan University\\nToronto, Canada\\nf1ibrahim@torontomu.ca\\nGuangjun Liu\\nDepartment of Aerospace Engineering\\nToronto Metropolitan University\\nToronto, Canada\\ngjliu@torontomu.ca\\nGuanghui Wang\\nDepartment of Computer Science\\nToronto Metropolitan University\\nToronto, Canada\\nwangcs@torontomu.ca\\nAbstract‚ÄîTransformers have become foundational for visual\\ntasks such as object detection, semantic segmentation, and\\nvideo understanding, but their quadratic complexity in attention\\nmechanisms presents scalability challenges. To address these\\nlimitations, the Mamba architecture utilizes state-space models\\n(SSMs) for linear scalability, efficient processing, and improved\\ncontextual awareness. This paper investigates Mamba architec-\\nture for visual domain applications and its recent advancements,\\nincluding Vision Mamba (ViM) and VideoMamba, which in-\\ntroduce bidirectional scanning, selective scanning mechanisms,\\nand spatiotemporal processing to enhance image and video un-\\nderstanding. Architectural innovations like position embeddings,\\ncross-scan modules, and hierarchical designs further optimize\\nthe Mamba framework for global and local feature extraction.\\nThese advancements position Mamba as a promising architecture\\nin computer vision research and applications.\\nIndex Terms‚ÄîMamba, State Space Models, Vision Mamba,\\nVideoMamba, Selective Scanning, Bidirectional Sequence Mod-\\neling.\\nI. INTRODUCTION\\nDeep learning has revolutionized artificial intelligence (AI),\\nand has been instrumental in advancing computer vision,\\nsignificantly enhancing machines‚Äô ability to interpret and\\nunderstand visual information from the world. By harness-\\ning the power of neural networks, particularly Convolutional\\nNeural Networks (CNNs) [1], [2] and, more recently, Vision\\nTransformers (ViTs) [3]‚Äì[5], deep learning has enabled the\\ndevelopment of complex, highly accurate models for tasks\\nsuch as image classification [6], object detection [7], semantic\\nsegmentation [8], [9], and spatio-temporal video understand-\\ning. [10], [11]. Transformers, originally popularized in natural\\nlanguage processing (NLP) [4], have shown significant poten-\\ntial in vision tasks. Architectures such as Vision Transformers\\n(ViTs) [5] and hybrid models [12], [13] leverage their ability\\nto capture complex dependencies, delivering state-of-the-art\\nresults.\\nAt the core of these advances are models with massive\\nparameter counts capable of capturing intricate patterns in\\ndata. Transformers despite their success, face challenges due\\nto the quadratic complexity of attention calculations, resulting\\nin time-consuming inference. Mamba, a novel architecture\\ninspired by state-space models (SSMs), presents a promising\\nalternative for building foundation models [14]. With near-\\nlinear scalability in sequence length and hardware-efficient\\nprocessing, Mamba has been recently adapted to computer vi-\\nsion tasks, including image and video processing. Models like\\nVision Mamba (ViM) [15] and VideoMamba [16] demonstrate\\nMamba‚Äôs ability to handle long-range dependencies while\\naddressing the computational and memory bottlenecks faced\\nby Transformer-based models in the visual domain.\\nThis paper offers a comprehensive review of Mamba archi-\\ntectures applied to visual tasks, highlighting their similarities,\\ndifferences, and effectiveness in addressing various challenges.\\nOur survey exclusively focuses on Mamba‚Äôs role in visual\\ntasks, setting it apart from broader Mamba surveys that cover\\napplications in text, recommendation systems, and medical\\nimaging [17]‚Äì[20]. While previous works like [21], [22]\\nexamine Mamba in vision, this paper uniquely focuses on\\nViM and VideoMamba as core models. We assess architec-\\ntural enhancements, including position embeddings, cross-scan\\nmodules, and hierarchical designs, to provide new insights into\\noptimizing Mamba for both local and global feature extraction.\\nAdditionally, we offer a comparative performance analysis\\nacross image classification, semantic segmentation, and object\\ndetection, giving practical recommendations for selecting the\\nbest architectures for various vision tasks. This task-specific\\ncomparison sets our work apart from general Mamba surveys\\nand provides valuable guidance for vision-based research.\\nThe rest of this survey is organized as follows: Section II\\nprovides an introductory overview of the building blocks of\\nViM and the theoretical foundations of SSMs. Section III\\ncompares ViM and Video Mamba. Section IV reviews recent\\nadvancements in Mamba architectures. Section V presents a\\nperformance comparison and analysis of the novel architec-\\ntures on common datasets. Section VI discusses the challenges\\nassociated with ViM and its applications. Finally, the paper is\\nconcluded in Section VII.\\nII. PRELIMINARY\\nA. Mamba Block\\nMamba enhances the context-aware capabilities of tradi-\\ntional SSMs by making its parameters functions of the input.\\nFig. 1 shows the original Mamba architecture block intended\\nfor 1D sequential data such as language tokens. SSMs map\\narXiv:2502.07161v1  [cs.CV]  11 Feb 2025\\nToken Embeddings\\nMamba Architecture\\nConv1d\\nSelective SSM\\n(State Space Model)\\nLin. Proj\\nLin. Proj\\nLin. Proj\\nSiLU\\nSiLU\\nMLP\\nFig. 1: Mamba Block Architecture with selective state space\\nmodels.\\na 1D function or sequence x(t) ‚ààR to an output y(t) ‚ààR\\nthrough a hidden state h(t) ‚ààRN. Using parameters A ‚àà\\nRN√óN, B ‚ààRN√ó1,C ‚ààR1√óN, the continuous system is\\ndefined by\\ndh(t)\\ndt\\n= Ah(t) + Bx(t),\\ny(t) = Ch(t)\\n(1)\\nFor the discrete sequences defined by the input x =\\n(x0, x1, . . . ) ‚ààRL, the continuous system parameters in\\nEq. (1) must be discretized using a step size ‚àÜ. Zero Order\\nHold (ZOH) [23] is used as the simplest and most reliable\\ndiscretization method in [15], [16], [24]. Using step size ‚àÜ,\\nthe continuous system parameters A, B are converted to ¬ØA, ¬ØB\\nas\\n¬ØA = exp(‚àÜA),\\n¬ØB = (‚àÜA)‚àí1(exp(‚àÜA) ‚àíI) ¬∑ ‚àÜB.\\n(2)\\nand the discretized system equation can be represented as\\nht = ¬ØAht‚àí1 + ¬ØBxt,\\nyt = Cht\\n(3)\\nB. Selective Scanning Mechanism\\nTraditional SSMs struggle with maintaining context aware-\\nness in dense text and data. Mamba introduces a selection\\nmechanism that dynamically adapts to input data, filtering ir-\\nrelevant information while retaining relevant data indefinitely.\\nMamba‚Äôs innovation lies in the Selective Scan Mechanism (S6)\\n[14], making SSM parameters B, C, ‚àÜfunctions of the input\\nas SB(x), SC(x), S‚àÜ(x). This is achieved through a linear\\ntransformation\\nB, C, ‚àÜ= Linear(x)\\nwhere B, C, ‚àÜ‚ààRB√óL√óN, L denotes the input sequence\\nlength, B is the batch size, and D denotes the number of\\nchannels.\\nThe original Mamba block [14] introduced a hardware-\\naware algorithm to efficiently compute the selective SSM\\nparameters. These parameters are used in SSM equations\\nEq. (1) and Eq. (3) to compute the latent hidden state and\\noutput. This mechanism allows Mamba to selectively retain\\nor discard information based on the relevance of the input\\nsequence.\\nht = ¬ØAht‚àí1 + ¬ØBxt,\\nyt = SCht\\n(4)\\n(a) Single Direction Scan\\n(b) Bidirectional Scan\\nFig. 2: Single direction scanning mechanism on patch embed-\\ndings shown in (a) follows the original Mamba architecture.\\nBidirectional Selective Scanning (b) introduces as a novel\\ncontribution in ViM.\\nTemporal-First\\nScanning\\n3D Bidirectional Spatial-First\\nScanning (Sweeping) across Patches\\nFig. 3: 3D Spatiotemporal Bidirectional Scanning. Video-\\nMamba [16] introduces an enhanced scanning mechanism for\\n3D input data to combine spatial data with temporal data.\\nwhere\\n¬ØA = exp(S‚àÜA)\\n¬ØB = (S‚àÜA)‚àí1(exp(S‚àÜA) ‚àíI) ¬∑ S‚àÜSB.\\n(5)\\nC. Vision Mamba (ViM) Block\\nVision Mamba extends the Mamba architecture to address\\nthe complexities of visual data. It tackles position sensitivity\\nby incorporating position embeddings, similar to those used in\\nViT [5]. To capture global context, ViM employs bidirectional\\nSSMs, as illustrated in Fig. 2, allowing each sequence element\\nto access information from both preceding and succeeding\\nelements [15].\\nD. Bidirectional/Spatiotemporal Scanning\\nThe original Mamba block‚Äôs unidirectional scanning suits\\ncausal sequences. However, visual data is non-causal. ViM\\nemploys bidirectional scanning, processing sequences forward\\nand backward to capture dependencies effectively, as shown\\nin Fig. 2. For video understanding, spatiotemporal scanning\\nextends this concept. VideoMamba employs 3D bidirectional\\nblocks to capture spatial and temporal dependencies, as shown\\nin Fig. 3. In addition to these strategies, researchers have\\nexplored various spatiotemporal methods, such as spatial-\\nfirst versus temporal-first scanning, as well as sweeping scan,\\ncontinuous scan, and local scan techniques, to further enhance\\nViM‚Äôs performance and efficiency. An overview of how these\\nmethods are utilized in key architectures is presented in\\nSection IV.\\nIII. VISION MAMBA VS. VIDEOMAMBA\\nThis section compares the Vision Mamba block with the\\nmore complex VideoMamba model, focusing on block design,\\nscanning mode, and memory management.\\n2\\nPatch Embeddings\\n(inc. Spatial Position)\\na\\nThe PlainMamba Block is repeated L times with a Global\\nPooling Layer\\xa0 at the end.\\nDepth-Wise\\nConv\\n\\xa0SSM x4\\nLin. Proj\\nLin. Proj\\nLin. Proj\\nDirection Aware Update\\n{ùõ©1,ùõ©2,ùõ©3,ùõ©4,ùõ©5}\\xa0\\nSiLU\\nSiLU\\nX\\nZ\\nMLP\\n(d) PlainMamba Block\\n(c) LocalMamba Architecture\\nPatch Embeddings\\n(inc. Spatial Position)\\nMLP\\nLayer\\nNorm\\nConv1d\\nLin. Proj\\nLin. Proj\\nX\\nZ\\nLin. Proj\\nSiLU\\nSiLU\\nLayer\\nNorm\\nSASF\\nFFN\\nLayer\\nNorm\\nThe Spatial Mamba Block is repeated 4 times in the Spatial Mamba\\narchitecture, with a downsampling between each block shown above.\\n(e) SpatialMamba Block\\nPatch Embeddings\\n(inc. Spatial Position / Cls Token)\\nMLP\\nLayer\\nNorm\\nForward\\nConv1d\\xa0\\nBackward\\nConv1d\\nLin. Proj\\nLin. Proj\\nX\\nZ\\nLin. Proj\\nForward Selective\\nSSM\\nBackward Selective\\nSSM\\nSiLU\\nToken\\nFusion\\nToken\\nFusion\\nToken Fusion Block; finds the most similar pairs of\\ntokens from two token sets, fuses the connected tokens.\\nPatch Embeddings\\n(inc. Spatial Position)\\nFFN\\nLayer\\nNorm\\nThe HiMamba Block is repeated 4 times in the\\xa0 HiMamba architecture, with each\\nincorporating a single direction scanning SSM. The head of the architecture\\nimplements an upsampling layer to produce High Resolution Output Image.\\nHxWxC\\nLayer\\nNorm\\nLayer\\nNorm\\nLocal\\nSSM\\nRegion\\nSSM\\nFusion\\nModule\\nH/n x W/n x C\\nConv\\nAvgPool\\nLin. Proj\\nSSM\\nSiLU\\nLayer\\nNorm\\nLocal SSM\\nRegion SSM\\nHxWxC\\nHxWxC\\nHxWxC\\n(H / n) x (W / n) x C\\nLin. Proj\\nLin. Proj\\n(H / n) x (W / n) x C\\nDepth-\\nWise Conv\\nSiLU\\nLin. Proj\\nSSM\\nSiLU\\nLayer\\nNorm\\nHxWxC\\nLin. Proj\\nLin. Proj\\nDepth-Wise\\nConv\\nSiLU\\nLin. Proj\\n(f) Famba-V Architecture\\n(g) Hi-Mamba Block\\nPatch Embeddings\\n(inc. Spatial Position / Cls Token)\\n(a) Vision Mamba (ViM) Architecture\\nLayer\\nNorm\\nForward\\nConv1d\\xa0\\nBackward\\nConv1d\\nLin. Proj\\nLin. Proj\\nX\\nZ\\nLin. Proj\\nForward Selective\\nSSM\\n(State Space Model)\\nBackward Selective SSM\\n(State Space Model)\\nSiLU\\nMLP\\nThe VSS Block is repeated 4 times in VMamba architecture, with\\na downsampling between each block shown above.\\n(b) VMamba Architecture (Visual State Space - VSS Block)\\nPatch Embeddings\\n(inc. Spatial Position / Cls Token)\\nLayer\\nNorm\\nDepth-Wise Conv\\nLin. Proj\\n2D-Selective Scan\\n(SS2D)\\nLayer\\nNorm\\nLin. Proj\\nLayer\\nNorm\\nFFN\\nSiLU\\nPatch Embeddings\\n(inc. Spatial Position / Cls Token)\\nMLP\\nConv1d\\nScan Direction 1\\nLin. Proj\\nLin. Proj\\nZ\\nX\\nLin. Proj\\nSelective SSM\\nConv1d\\nScan Direction 2\\nConv1d\\nScan Direction 3\\nConv1d\\nScan Direction 4\\nSpatial & Channel\\nAttention Module\\n(SCAttn)\\nFC\\nFC\\nSiLU\\nSiLU\\nConv\\nAvgPool\\nFC\\nGlobal\\nGELU\\nFC\\nLocal\\nGELU\\nSpatial Attn\\nChannel\\nAttn\\nSiLU\\nFig. 4: Comparison of Mamba architectures for 2D image applications; (a) Vision Mamba, (b) Vmamba, (c) LocalMamba\\nis a set of multidirectional blocks that can be applied to other Mamba architectures, (d) PlainMamba, (e) SpatialMamba, (f)\\nFamba-V introduces the concept of token fusion that can be applied to other Mamba architectures, (g) Hi-Mamba, used to\\nextract high-resolution image features and for high-resolution image restoration.\\nA. Vision Mamba (ViM)\\nThe ViM block as shown in Fig. 4(a) is an adaptation of\\nthe Mamba block designed to process visual data using bidi-\\nrectional scanning/sequence modeling. Unlike the original\\nunidirectional Mamba block the ViM block processes flattened\\nvisual sequences with simultaneous forward and backward\\nSSMs to capture spatial context. The input sequence is gen-\\nerated by transforming the 2D image into flattened 2D image\\npatches with the addition of both position embeddings and\\nclass tokens; the latter represents the whole patch sequence.\\nThe normalized sequence input is linearly projected to\\nproduce two transformed representations: x (used for main\\nprocessing) and z (used later for gating). The x transformed\\nrepresentation is processed in both forward and backward\\ndirections (similar to bidirectional RNNs or Transformers). For\\neach direction:\\n‚Ä¢ A 1D convolution is applied to x, followed by a SiLU\\nactivation, producing x‚Ä≤\\no; represented by the ‚ÄùForward\\nConv1d and Backward Conv1d‚Äù blocks in Fig. 4(a).\\n‚Ä¢ Within the SSM blocks, three separate linear projections\\ntransform x‚Ä≤\\no into: Bo, Co, ‚àÜo.\\n‚Ä¢ The scaling factor ‚àÜo is applied to transform Ao, Bo:\\n3\\n¬Ø\\nAo = exp(‚àÜoAo),\\n¬Ø\\nBo = (‚àÜoAo)‚àí1(exp(‚àÜoAo) ‚àíI) ¬∑ ‚àÜoBo.\\n(6)\\n‚Ä¢ The SSM then computes forward-scanned (yforward and\\nbackward-scanned ybackward) output features, which are\\ngated and averaged to produce the output.\\nThe outputs (yforward and ybackward) are modulated using\\nSiLU activations applied to z; This acts as a gating mecha-\\nnism, controlling the contribution of each direction. The gated\\noutputs are added together and passed through a final linear\\ntransformation, where a residual connection (adding back\\nthe original normalized sequence input ensures that useful\\noriginal information is retained.\\nThis bidirectional processing is a key difference from the\\noriginal Mamba block, allowing the ViM block to consider the\\ncontext from both past and future tokens, as image data is in-\\nherently non-causal, and a token in a future sequence segment\\nmay have significant relevance to the context of the currently\\nassessed image token. The ViM block incorporates position\\nembeddings to preserve spatial information, addressing the\\noriginal Mamba‚Äôs lack of spatial awareness as it was designed\\nfor 1D sequences. Additionally, parameters are shared for\\nboth forward and backward scanning directions within the\\nViM block. By incorporating bidirectional SSMs and position\\nembeddings, ViM and all of its variants aim to achieve com-\\nparable modeling power to Transformers while maintaining a\\nlinear memory complexity and efficiently compressing visual\\nrepresentations, making it suitable for various vision tasks.\\nWhile effective, ViM faces the following unique limitations:\\n‚Ä¢ Presence of Artifacts: One study reveals that ViM feature\\nmaps can contain artifacts, potentially impacting perfor-\\nmance [25]. These artifacts necessitate further refine-\\nments to the architecture for optimal results.\\n‚Ä¢ Mitigating Artifacts with Mamba¬Æ: To address artifacts,\\nresearchers propose Mamba¬Æ, which strategically incor-\\nporates ‚Äùregister tokens‚Äù evenly distributed throughout\\nthe token sequence. These tokens are then concatenated\\nat the end of the network to generate a comprehensive\\nimage representation. Mamba¬Æ effectively reduces arti-\\nfacts, sharpens the model‚Äôs focus on semantically relevant\\nimage areas, and surpasses previous models in accuracy\\nand scalability [25].\\n‚Ä¢ Limited global and local representations concurrently:\\nResearchers have noted that the unidirectional nature of\\ntraditional Vision Mamba architectures can limit their\\nability to effectively capture global representations, po-\\ntentially reducing performance in tasks requiring com-\\nprehensive spatial context. [26], [27].\\nThere have been several ViM variations designed to address\\nthese specific challenges and more to enhance performance\\nacross various tasks:\\n‚Ä¢ Vmamba: This model introduces the Cross-Scan Module\\n(CSM) to specifically address the two-dimensional nature\\nof images. By considering spatial relationships on a 2D\\nplane as other models have also done [28], Vmamba aims\\nto improve the efficiency of visual processing [24].\\n‚Ä¢ LocalMamba and EfficientVMamba: These variations fo-\\ncus on optimizing scanning strategies within the Mamba\\narchitecture. LocalMamba employs window scanning\\nand dynamic scanning directions [27], while EfficientV-\\nMamba integrates atrous-based selective scanning and\\ndual-pathway modules for efficient global and local fea-\\nture extraction [29].\\n‚Ä¢ VSSD Mamba: This model addresses the fundamental\\nchallenge of adapting the causal nature of traditional\\nSSD/SSM architectures to inherently non-causal vision\\ntasks [30]. It overcomes this by disregarding the magni-\\ntude of interactions between hidden states and tokens,\\nretaining only their relative weights. This technique,\\nalong with multi-scan strategies, enables the non-causal\\nintegration of scanning results, significantly improving\\nperformance in vision applications [26].\\nB. Video Mamba\\nVideoMamba is designed specifically for video understand-\\ning, addressing the challenges of local redundancy and global\\ndependencies in video data. It extends the ViM block for\\n3D video sequences, incorporating a bidirectional Mamba\\nblock, and employs various scanning methods, such as Spatial-\\nFirst, Temporal-First, and Spatiotemporal scans, to effec-\\ntively process spatiotemporal information, as illustrated in\\nFig. 3. The model incorporates a linear-complexity operator\\nto handle long-term dependencies efficiently and includes self-\\ndistillation, where a smaller model acts as a teacher, guiding\\nthe training of the larger model to improve scalability.\\nVideoMamba [16] extracts key frames from videos and\\nprocesses them as continuous input sequences. However, its\\nperformance on video benchmarks falls short of transformer-\\nbased methods, indicating room for improvement [31]. Build-\\ning upon VideoMamba, [32] addresses the limitations of ‚Äúhis-\\ntorical decay‚Äù and ‚Äúelement contradiction‚Äù. Masked backward\\ncomputation mitigates the historical decay issue, allowing\\nthe network to better handle historical tokens. Extensions of\\nVideoMamba models [32], [33] implement residual connec-\\ntions to refine the representation of similarity between tokens.\\nThese enhancements significantly boost VideoMamba‚Äôs perfor-\\nmance on video action recognition benchmarks, establishing\\nit as a strong competitor to transformer models.\\nWhile both models leverage bidirectional SSMs, Video-\\nMamba extends this capability to spatiotemporal processing\\nand incorporates distinct scanning methods, masked modeling\\nstrategies, and self-distillation for enhanced performance.\\nThe architectures of two VideoMamba models are de-\\npicted in Fig. 5. VideoMamba and its refined variants have\\nbeen tested against video transformer architectures on tasks\\nsuch as action recognition, video understanding, and multi-\\nmodal tasks [16]. A comparative efficiency analysis between\\nVideoMamba and transformer-based models reveals significant\\nimprovements in scalability and performance. [11], [16], [31]‚Äì\\n[34] highlights Mamba‚Äôs speed advantages when processing\\n4\\nLayer\\nNorm\\nForward\\nConv1d\\xa0\\nBackward\\nConv1d\\nLin. Proj\\nLin. Proj\\nX\\nZ\\nLin. Proj\\nForward Selective SSM\\n(State Space Model)\\nBackward Selective SSM\\n(State Space Model)\\nSiLU\\n3D Patch Embeddings\\n(inc. Spatial Position / Temporal\\nPosition / Cls Token)\\n(a) VideoMamba (Bidirectional Mamba)\\nMLP\\nD Patch Embeddings\\npatial Position / Temporal\\nPosition / Cls Token)\\nMLP\\nLayer\\nNorm\\nForward\\nConv1d\\xa0\\nLin. Proj\\nX\\nLin. Proj\\nForward Residual\\nSSM\\nMasked Backward\\nResidual SSM\\nBackward\\nConv1d\\n(b) VideoMambaPro (Bidirectional Mamba w/ Residual\\nSSM Blocks)\\n3D Patch Embeddings\\n(inc. Spatial Position / Temporal\\nPosition / Cls Token)\\nMLP\\nLayer\\nNorm\\nForward\\nConv1d\\xa0\\nLin. Proj\\nX\\nLin. Proj\\nForward Residual\\nSSM\\nMasked Backward\\nResidual SSM\\nBackward\\nConv1d\\n(b) VideoMambaPro (Bidirectional Mamba w/ Residual\\nSSM Blocks)\\nFig. 5: (a) VideoMamba Block and (b) VideoMambaPro\\nArchitectures. Both employ similar architectures while (a) is\\nthe same as [15] adapted for 3D data, (b) uses the concept of\\nresidual SSM and masked patches in the backward direction\\nto improve on the architecture in (a).\\nvideos with a large number of frames, making Mamba a\\nparticularly attractive choice for high-volume video processing\\napplications.\\nC. Comparison\\nViM [15] and VideoMamba [16] both adapt the Mamba\\nstate space model (SSM) [14] for visual tasks. ViM serves as\\na generic vision backbone designed to replace traditional con-\\nvolutional and attention-based networks [4]. It processes im-\\nages as sequences of patches, utilizing bidirectional scanning\\nto efficiently capture spatial context. ViM normalizes input\\ntokens, applies linear projections, and employs a bidirectional\\nSSM with position embeddings to enhance spatial information\\nrepresentation.\\nVideoMamba extends ViM to 3D video sequences, incor-\\nporating multiple scanning methods, including spatial-first,\\ntemporal-first, and spatiotemporal scanning. Unlike ViM, it\\nfollows a pure SSM-based architecture similar to the vanilla\\nViT [5], without the use of downsampling layers. VideoMamba\\nalso introduces a linear-complexity operator to effectively\\nmodel long-term dependencies, making it particularly suitable\\nfor processing spatiotemporal data in video tasks. Both models\\nhave been evaluated on tasks like image classification [35],\\nobject detection [36], semantic segmentation [37]‚Äì[39], and\\nmany other vision tasks [34], [40].\\nIn essence, ViM serves as a general-purpose image process-\\ning backbone, while VideoMamba is specifically tailored for\\nefficient video understanding, with architectural adjustments\\ndesigned to address the unique characteristics and challenges\\nof their respective data types and tasks.\\nIV. KEY ARCHITECTURES\\nThis section summarizes the recent advancements in Vi-\\nsion/Video Mamba-based studies from the perspectives of key\\narchitectural innovations. Fig. 6 shows additional selective\\n(a) Sweeping /\\xa0Raster\\nScanning\\n(b) Continuous 2D /\\nZigzag\\xa0Scanning\\n(c) Local Scan\\n(e) Atrous / Efficient 2D Scan\\n(d) Wavefront Scan\\nFig. 6: Scanning Mechanisms; (a) and (b) are applied on\\npatches in the forward and backward directions, while (a) uses\\nraster scanning, which loses causal information that (b) retains\\nfrom patches located vertically from each other. Local Scan\\n(c) divides an image into local segments for improved local\\nrepresentation. Scans shown in (d) and (e) are not commonly\\nused scanning mechanisms but are efficient alternatives to (a)\\nand (b).\\nscan mechanisms employed by the architecture variants de-\\nscribed below. Each novel scanning strategy has been config-\\nured to supplement using Mamba for 2D and 3D applications,\\nsuch as spatial relationship modeling, limited local repre-\\nsentations, and varying directional scanning for better visual\\ncontextual features. Fig. 4 shows visual block representations\\nof some of the key architectures used in image tasks and how\\nthese blocks compare to the original Mamba block in Fig. 1.\\na) Selective State Space Models (SSMs): This is the\\nfoundation of the Mamba architecture. SSMs, especially the S6\\nblock, offer a global receptive field and linear complexity in\\nrelation to sequence length, making them a computationally\\nefficient alternative to traditional Transformers [14], [26],\\n[29], [41]. Mamba introduces a selection mechanism, enabling\\nthe model to selectively retain or discard information based\\non the input, enhancing context-based reasoning [14], [41].\\nThis architecture enables the model to perform content-based\\nreasoning by making the SSM parameters functions of the\\ninput. This allows the models to selectively propagate or forget\\ninformation along the sequence length, filtering irrelevant\\ninformation and maintaining long-term memory of relevant\\ninformation [14], [30], [41], [42].\\nb) Multi-path Scanning Mechanisms: To overcome the\\nlimitations of Mamba‚Äôs causal nature, which is not ideal for\\nimage processing, several models like VMamba in Fig.4(b),\\nLocalMamba in Fig.4(c), and PlainMamba in Fig.4(d) have\\nadopted multi-path scanning mechanisms. This allows the\\nmodels to capture both local and global features more effec-\\ntively [15], [27], [43].\\nLocalMamba [27] improves image scanning by dividing\\nimages into distinct local windows, ensuring relevant tokens\\nare closely arranged to better capture local dependencies while\\nmaintaining spatial coherence. To incorporate global context,\\nthe method integrates a selective scan mechanism across four\\ndirections: the original two and their flipped counterparts,\\nallowing for both forward and backward token processing, as\\n5\\nshown in Fig. 6(c).\\nc) Structure-Aware SSM Layer: Introduced in Spatial-\\nMamba, this layer incorporates a Structure-Aware State Fusion\\n(SASF) branch and a multiplicative gate branch as shown in\\nFig. 4(e). This allows the model to capture both local and\\nglobal spatial dependencies [44]. Unlike previous methods that\\nrely on multiple scanning directions, Spatial-Mamba fuses the\\nstate variables into a new structure-aware state variable [44].\\nOnce an input image is flattened into 1D patches and state\\nparameters are determined, the patches are reshaped into 2D,\\nand SASF is applied to integrate local dependencies before\\ngenerating the final output.\\nd) Cross-Layer Token Fusion: Famba-V, a technique\\ndesigned to enhance the training efficiency of Vision Mamba\\n(ViM) models, incorporates three cross-layer token fusion\\nstrategies [45]‚Äì[48].\\nToken fusion is applied within the ViM block as shown\\nbefore the final linear projection operation in Fig. 4 (f), the\\ntokens are grouped and paired based on cosine similarity, and\\nfused by averaging. There are three token fusion strategies\\nimplemented in [45] with upper layer token fusion, producing\\nthe best results on vision tasks shown in Section V:\\n‚Ä¢ Interleaved Token Fusion ‚Äì Applies fusion to alternate\\nlayers, starting from the second layer, to maintain effi-\\nciency without overly aggressive fusion.\\n‚Ä¢ Lower-layer Token Fusion ‚Äì Applies fusion only to the\\nlower layers, leveraging early-stage efficiency gains while\\npreserving high-level reasoning.\\n‚Ä¢ Upper-layer Token Fusion ‚Äì Applies fusion only to upper\\nlayers, assuming high-level features can be fused without\\nsignificantly affecting performance.\\ne) Hierarchical Mamba Architecture (Hi-Mamba): This\\narchitecture in Fig. 4(g) designed for image super-resolution,\\nintroduces a hierarchical Mamba block (HMB) comprised of\\nlocal and region SSMs with single-direction scanning. This\\nsetup aims to capture multi-scale visual context efficiently\\n[49].\\nf) Direction Alternation Hierarchical Mamba Group\\n(DA-HMG): Within Hi-Mamba, this group further enhances\\nspatial relationship modeling. It allocates single-direction\\nscanning into cascaded HMBs, improving performance with-\\nout increasing computational costs or parameters [49].\\ng) Non-Causal State Space Duality (NC-SSD): Models\\nlike VSSD introduce non-causal SSD, eliminating the causal\\nmask in the state space dual model (SSD) of Mamba2 [30].\\nThis architectural innovation allows for a more flexible ap-\\nplication of Mamba to inherently non-causal data like images\\n[15], [26], [29], [50].\\nThe VSSD block [26] enhances Mamba2 SSD for vision\\ntasks by incorporating Depth-Wise Convolution (DWConv)\\n[51], a Feed-Forward Network (FFN) for better channel com-\\nmunication, and a Local Perception Unit (LPU) for improved\\nlocal feature extraction. The four-stage hierarchical VSSD\\nmodel uses VSSD blocks in the first three stages and Multi-\\nHead Self-Attention (MSA) in the last, balancing efficiency\\nand performance in vision applications [4].\\nh) Hidden State Mixer-based SSD (HSM-SSD): Effi-\\ncientViM models feature this layer, which shifts the channel\\nmixing operations from the image feature space to the hidden\\nstate space. This design aims to alleviate the computational\\nbottleneck of the standard SSD layer without compromising\\nthe model‚Äôs ability to generalize [29].\\ni) Register Tokens in Vision Mamba: Mamba¬Æ intro-\\nduces registers, input-independent tokens, into the Vision\\nMamba architecture. These tokens, inserted evenly throughout\\nthe input and recycled for final predictions, aim to mitigate\\nartifacts and enhance the model‚Äôs focus on semantically mean-\\ningful image regions [25].\\nj) Masked Backward Computation: VideoMambaPro in-\\ncorporates masked backward computation during the bidirec-\\ntional Mamba process, mitigating the issue of historical decay\\nin token processing and enabling the model to better utilize\\nhistorical information [16], [32].\\nk) Elemental Residual Connections:\\nVideoMambaPro\\nalso introduces residual connections to the matrix elements\\nof Mamba, addressing the challenge of element contradiction,\\nwhere elements in the sequence can conflict during computa-\\ntion. This enhances the model‚Äôs ability to extract and process\\ncomplex spatio-temporal features in video data [16], [32].\\nMany of these innovations focus on enhancing the effi-\\nciency and performance of Mamba-based models in vision\\ntasks, while preserving the linear complexity advantage over\\ntraditional Transformers.\\nV. PERFORMANCE COMPARISON\\nTo provide a systematic comparison of the performance of\\ndifferent models across various datasets and tasks, this section\\ncompares the results of [14]‚Äì[16], [24]‚Äì[27], [29], [30], [32],\\n[43]‚Äì[45] in common dataset benchmarks. The models shown\\nare only those of a similar size in the cases where researchers\\nprovided ‚Äòsmall‚Äô, ‚Äòbase‚Äô, or ‚Äòlarge‚Äô model variations. The re-\\nsults are categorized by tasks; image classification against the\\nImagenet-1k dataset [35], [52], semantic segmentation against\\nthe ADE20K dataset [38], [53], object detection against the\\nMS-COCO dataset [54], and human action recognition for\\nvideo understanding benchmarks against both Kinetics-400\\n[55], [56] and Something-Something-V2 [57] datasets.\\nA. Image Classification\\nTable I compares various Mamba models used for image\\nclassification [10], [35] on the ImageNet-1k dataset [52],\\nemphasizing differences in scanning mechanisms, accuracy,\\nand efficiency. From the results we can see that Spatial-\\nMamba-S (84.6% Top-1 ACC) [44] is the best performer,\\nleveraging Structure-Aware State Fusion, while Vmamba-S\\n(83.6%) [24] and LocalMamba applied to Vmamba-S (83.7%)\\n[15], [27] also achieve strong results using cross-scan and local\\nscan. Famba-V applied to ViM-S (75.2%) [15], [45] performs\\nthe worst, although tested on a different dataset, suggests\\nbidirectional scanning with token fusion is less effective for\\nimage classification. In terms of efficiency, EfficientViM-M4\\n(21.3M params, 4.1 GFLOPS, 81.9% ACC) [29] provides an\\n6\\nTABLE I: Comparison of Different Image Classification Model Results\\nModel\\nData\\nTask\\nScanning Mechanism\\n#Params (M)\\nFLOPS (G)\\nTop-1 ACC\\nVision Mamba (ViM-S) [15]\\nImageNet-1k\\nImage Classification\\nBidirectional Scan (Sweeping / Raster Scan)\\n26\\n-\\n80.3\\nVmamba-S [24]\\nImageNet-1k\\nImage Classification\\nCross-Scan (Raster Scanning along 4 directions)\\n50\\n8.7\\n83.6\\nSpatial Mamba-S [44]\\nImageNet-1k\\nImage Classification\\nSweeping Scan with Structure Aware State Fusion\\n43\\n7.1\\n84.6\\nFamba-V applied to ViM-S [45]\\nCIFAR-100\\nImage Classification\\nBidirectional Scan w/ Token Fusion\\n26\\n-\\n75.2\\nLocalMamba applied to ViM-S [27]\\nImageNet-1k\\nImage Classification\\nLocal Scan (Sweeping/ Raster Scan)\\n28\\n4.8\\n81.2\\nLocalMamba applied to Vmamba-S [27]\\nImageNet-1k\\nImage Classification\\nLocal Scan (Sweeping/ Raster Scan)\\n50\\n11.4\\n83.7\\nVSSD-S (based on Mamba2) [26]\\nImageNet-1k\\nImage Classification\\nBidirectional Scan w/ Non-Causal State Space Duality\\n40\\n7.4\\n84.1\\nEfficientViM-M4 [29]\\nImageNet-1k\\nImage Classification\\nBidirectional Scan w/ Hidden State Mixer SSD\\n21.3\\n4.1\\n81.9\\nPlainMamba-L2 [43]\\nImageNet-1k\\nImage Classification\\nContinuous 2D / Zigzag Scan\\n25.7\\n8.1\\n81.6\\nMamba¬Æ-S [25]\\nImageNet-1k\\nImage Classification\\nBidirectional Scan w/ evenly distributed Register Token Embeddings\\n28\\n9.9\\n81.1\\nTABLE II: Comparison of Different Semantic Segmentation Model Results\\nBackbone with Upernet Model\\nData\\nTask\\nScanning Mechanism\\n#Params (M)\\nFLOPS (G)\\nmIoU\\nVision Mamba (ViM-S) [15]\\nADE20K\\nSemantic Segmentation\\nBidirectional Scan (Sweeping / Raster Scan)\\n46\\n-\\n44.9\\nVmamba-S [24]\\nADE20K\\nSemantic Segmentation\\nCross-Scan (Raster Scanning along 4 directions)\\n82\\n1028\\n50.6\\nSpatial Mamba-S [44]\\nADE20K\\nSemantic Segmentation\\nSweeping Scan with Structure Aware State Fusion\\n73\\n992\\n50.6\\nLocalMamba applied to ViM-S [27]\\nADE20K\\nSemantic Segmentation\\nLocal Scan (Sweeping/ Raster Scan)\\n58\\n297\\n46.4\\nLocalMamba applied to Vmamba-S [27]\\nADE20K\\nSemantic Segmentation\\nLocal Scan (Sweeping/ Raster Scan)\\n81\\n1095\\n50\\nVSSD-T (based on Mamba2) [26]\\nADE20K\\nSemantic Segmentation\\nBidirectional Scan w/ Non-Causal State Space Duality\\n53\\n941\\n47.9\\nPlainMamba-L2 [43]\\nADE20K\\nSemantic Segmentation\\nContinuous 2D / Zigzag Scan\\n55\\n285\\n46.8\\nMamba¬Æ [25]\\nADE20K\\nSemantic Segmentation\\nBidirectional Scan w/ evenly distributed Register Token Embeddings\\n56\\n-\\n45.3\\nexcellent accuracy-to-computation trade-off, making it ideal\\nfor resource-constrained applications.\\nThe results show that advanced state-space fusion (Spatial-\\nMamba-S) and strategic scanning mechanisms (Vmamba-\\nS,\\nLocalMamba)\\nsignificantly\\nenhance\\naccuracy,\\nwhile\\nlightweight architectures (EfficientViM-M4) [29] can balance\\nefficiency and accuracy effectively.\\nB. Semantic Segmentation\\nTable II compares the reviewed Mamba models on the task\\nof semantic segmentation on ADE20K [37]‚Äì[39], [53]. The\\nresults show that Vmamba-S [24] and Spatial-Mamba-S (50.6\\nmIoU) [44] achieve the best performance, leveraging cross-\\nscan and Structure-Aware State Fusion, though they demand\\nhigh computational costs (1028G and 992G FLOPS). Local-\\nMamba applied to Vmamba-S (50.0 mIoU, 1095G FLOPS)\\nalso performs well [27], emphasizing local scanning benefits.\\nMambaR-S (45.3 mIoU) performs the worst, showing register\\ntoken embeddings are less effective for segmentation [25].\\nVSSD-T (47.9 mIoU), although only tested with a ‚Äòtiny‚Äô\\nversion of the model, balances accuracy and efficiency with\\nnon-causal state space duality [26].\\nThe results highlight the trade-offs between scanning mech-\\nanisms, mIoU, and efficiency (Params and FLOPS). This\\nanalysis indicates that high-performing models depend on ad-\\nvanced scanning techniques but demand considerable compu-\\ntational resources, while lighter models, such as PlainMamba-\\nL2 [43], sacrifice some accuracy for improved efficiency.\\nC. Object Detection\\nTable\\nIII evaluates object detection models [36] on MS-\\nCOCO [51], [54], comparing accuracy based on 75% over-\\nlapping bounding box with ground truth labels (APbb75) and\\nefficiency (Params and FLOPS).\\nSpatial Mamba-S (54.2 APbb75) is the best performer, bene-\\nfiting from Sweeping Scan with Structure-Aware State Fusion,\\nalthough it has high computational demands (315G FLOPS)\\n[44]. VSSD-S (53.1 APbb75) follows closely, leveraging Non-\\nCausal State Space Duality, balancing accuracy and efficiency\\n(325G FLOPS) [26]. LocalMamba applied to Vmamba-S\\n(52.7 APbb75) also performs well, emphasizing local scanning\\nadvantages [27]. The worst performer is EfficientViM-M4\\n(41.1 APbb75), which, despite its efficiency (4.1G FLOPS),\\nsacrifices accuracy in place of reduced computational cost\\n[29]. Vmamba-S (48.7 APbb75) and Vision Mamba (49.6\\nAPbb75) [15], [24], which rely on cross-scan and bidirec-\\ntional raster scanning, show moderate performance but are\\nless effective than structure-aware or local scanning methods.\\nPlainMamba-L2 (50.1 APbb75, 542G FLOPS) [43] achieves\\ndecent results but is computationally expensive. This analysis\\nhighlights that Spatial Mamba-S and VSSD-S lead in accuracy,\\nwhile EfficientViM-M4 prioritizes efficiency at the cost of\\nperformance.\\nD. Human Action Recognition\\nTable\\nIV\\ncompares\\nthe\\nVideoMamba-S\\n[16]\\nand\\nVideoMambaPro-S [32] models tested on the Kinetics-\\n400 [55] and SSV2 [57] datasets for action recognition.\\nVideoMambaPro-S emerges as the superior model, achieving\\nthe highest accuracy of 88.5% on Kinetics-400 while using\\nfewer parameters (24M vs 26M) than VideoMamba-S. Both\\nmodels consistently perform better on Kinetics-400 than\\nSSV2. Ref. [32] utilizes an enhanced scanning mechanism,\\nwhich\\nincludes\\nadditional\\nResidual\\nSSM\\nand\\nMasked\\nBackward Residual SSM components [32], [51], [58], which\\ncontributed\\nto\\nbetter\\nperformance\\nacross\\nboth\\ndatasets.\\nAlthough computational costs (FLOPS) are significantly\\nlower for SSV2 experiments (400G) compared to Kinetics-\\n400 (1500-1620G), the worst-performing configuration is\\nVideoMamba-S in SSV2 with 67. 6% accuracy. Overall,\\nthe Pro version demonstrates that its more sophisticated\\narchitecture delivers better results with slightly improved\\nefficiency.\\nE. Discussion\\nBased on the above experimental evaluations, it is evident\\nthat Spatial Mamba-S and Vmamba-S are the best choices\\nfor image classification (84.6% and 83.6% Top-1 ACC) and\\n7\\nTABLE III: Comparison of Different Object Detection Model Results\\nBackbone with Mask R-CNN Model\\nData\\nTask\\nScanning Mechanism\\n#Params (M)\\nFLOPS (G)\\nAPbox\\n75\\nVision Mamba (ViM-Ti) [15]\\nMS-COCO\\nObject Detection\\nBidirectional Scan (Sweeping / Raster Scan)\\n49.6\\n-\\n-\\nVmamba-S [24]\\nMS-COCO\\nObject Detection\\nCross-Scan (Raster Scanning along 4 directions)\\n70\\n349\\n48.7\\nSpatial Mamba-S [44]\\nMS-COCO\\nObject Detection\\nSweeping Scan with Structure Aware State Fusion\\n63\\n315\\n54.2\\nLocalMamba applied to Vmamba-S [27]\\nMS-COCO\\nObject Detection\\nLocal Scan (Sweeping/ Raster Scan)\\n69\\n414\\n52.7\\nVSSD-S (based on Mamba2) [26]\\nMS-COCO\\nObject Detection\\nBidirectional Scan w/ Non-Causal State Space Duality\\n59\\n325\\n53.1\\nEfficientViM-M4 [29]\\nMS-COCO\\nObject Detection\\nBidirectional Scan w/ Hidden State Mixer SSD\\n21.3\\n4.1\\n41.1\\nPlainMamba-L2 [43]\\nMS-COCO\\nObject Detection\\nContinuous 2D / Zigzag Scan\\n53\\n542\\n50.1\\nTABLE IV: Comparison of Different Video Understanding ‚Äì Human Action Recognition Model Results\\nBackbone/Model\\nData\\nTask\\nScanning Mechanism\\n#Params (M)\\nFLOPS (G)\\nTop-1 ACC\\nVideoMamba-S [16]\\nKinetics-400\\nAction Recognition\\n3D Bidirectional Scan\\n26\\n1620\\n81.5\\nVideoMamba-S [16]\\nSSv2\\nAction Recognition\\n3D Bidirectional Scan\\n26\\n408\\n67.6\\nVideoMambaPro-S [32]\\nKinetics-400\\nAction Recognition\\n3D Bidirectional Scan w/ Residual SSM + Masked Backward Residual SSM\\n24\\n1500\\n88.5\\nVideoMambaPro-S [32]\\nSSv2\\nAction Recognition\\n3D Bidirectional Scan w/ Residual SSM + Masked Backward Residual SSM\\n24\\n400\\n74.3\\nsemantic segmentation (50.6 mIoU), though they require high\\ncomputational cost. For object detection, Spatial Mamba-S\\n(54.2 APbb75) and VSSD-S (53.1 APbb75) perform best, lever-\\naging structure-aware and non-causal mechanisms. VSSD-\\nS and LocalMamba offer a balance between accuracy and\\nefficiency across tasks.\\nVI. CHALLENGES AND FUTURE RESEARCH\\nMamba models are capable of modeling long-range depen-\\ndencies with linear complexity, making them well-suited for\\ntasks involving high-resolution images, long sequences, and\\nlarge video datasets. Techniques such as bidirectional scanning\\nand multi-scale processing further enhance Mamba‚Äôs ability\\nto capture both local and global context. Mamba is currently\\nbeing explored in specialized vision domains, such as:\\n‚Ä¢ Scene flow estimation. FlowMamba [59] uses an iterative\\nSSM-based update module and a feature-induced order-\\ning strategy to capture long-range motion.\\n‚Ä¢ Vein recognition. Global-local Vision Mamba (GLVM)\\ncombines local and global feature learning for enhanced\\nvein recognition [40].\\n‚Ä¢ Medical image analysis. PV-SSM is a pure visual state\\nspace model achieving strong results across medical\\nimage analysis [60].\\n‚Ä¢ Skin lesion segmentation. MambaU-Lite [61] combines\\nMamba and CNN architectures for efficient skin lesion\\nsegmentation, using a novel P-Mamba block.\\nFuture research aims to develop native 2D and 3D Mamba\\nmodels, non-causal state-space dual models, and efficient\\ntoken fusion techniques. Some potential directions include:\\n‚Ä¢ Multi-modal image fusion. Shuffle Mamba employs a\\nrandom shuffle-based scanning method to mitigate biases\\nin multi-modal image fusion [62].\\n‚Ä¢ RGB-D salient object detection. MambaSOD is a dual\\nMamba-driven network for RGB-D salient object detec-\\ntion, using cross-modal fusion to combine RGB and depth\\ninformation [63].\\n‚Ä¢ Point cloud processing. NIMBA reorders point cloud\\ndata to maintain 3D spatial structure, enhancing Mamba‚Äôs\\nsequential processing [64]. Serialized Point Mamba [65]\\nuses a state-space model for dynamic compression of\\npoint cloud sequences for efficient segmentation.\\nCombining Mamba with CNNs and Transformers is also a\\nresearch direction to compensate for each other‚Äôs weaknesses.\\n‚Ä¢ Object detection with YOLO. Mamba-YOLO integrates\\nMamba into the YOLO architecture, achieving improved\\nobject detection performance [66].\\n‚Ä¢ 3D object detection. MambaBEV [42] and PillarMamba\\n[67] integrate Mamba into 3D object detection models\\nfor autonomous driving. MambaDETR is also used for\\n3D object detection [68].\\n‚Ä¢ Point cloud enhancement. MambaTron uses Mamba for\\ncross-modal point cloud completion, combining Mamba‚Äôs\\nefficiency with Transformer‚Äôs analytical capabilities [69].\\nVideoMamba [16] still has challenges in achieving perfor-\\nmance parity with Transformers, addressing historical decay,\\nand managing potential element contradictions [70]. Future\\nresearch will focus on enhancing stability and closing the\\nperformance gap with other architectures, in order to fully\\nunlock Mamba‚Äôs potential for efficient, long-range context\\nmodeling in vision and video.\\nVII. CONCLUSION\\nThis paper provides an overview of visual Mamba, detail-\\ning their architectures, applications, and challenges. Vision\\nMamba functions as a general vision backbone, processing\\nimages as sequences of patches with bidirectional scanning\\nto effectively capture spatial context. Video Mamba extends\\nthis approach to 3D video sequences, operating across both\\nspatial and temporal dimensions. Despite significant advance-\\nments and key architectural developments, challenges persist\\nin fully realizing Mamba‚Äôs potential for vision tasks. Future re-\\nsearch should focus on refining these architectures, broadening\\nMamba‚Äôs applicability to a wider range of visual modalities,\\ndeveloping efficient token fusion techniques, and improving\\nits scalability for real-world deployment.\\nACKNOWLEDGMENT\\nThis research was funded by the Natural Sciences and\\nEngineering Research Council of Canada (NSERC) under\\ngrant numbers ALLRP 588173-23 and ALLRP 570580-21.\\n8\\nREFERENCES\\n[1] Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey, ‚ÄúWhat makes con-\\nvolutional models great on long sequence modeling?‚Äù arXiv preprint\\narXiv:2210.09298, 2022.\\n[2] V. Lioutas and Y. Guo, ‚ÄúTime-aware large kernel convolutions,‚Äù in\\nInternational Conference on Machine Learning.\\nPMLR, 2020, pp.\\n6172‚Äì6183.\\n[3] T. Zhang, W. Xu, B. Luo, and G. Wang, ‚ÄúDepth-wise convolutions in\\nvision transformers for efficient training on small datasets,‚Äù Neurocom-\\nputing, vol. 617, p. 128998, 2025.\\n[4] A. Vaswani, ‚ÄúAttention is all you need,‚Äù Advances in Neural Information\\nProcessing Systems, 2017.\\n[5] A. Dosovitskiy, ‚ÄúAn image is worth 16x16 words: Transformers for\\nimage recognition at scale,‚Äù arXiv preprint arXiv:2010.11929, 2020.\\n[6] R. Alapati, B. Renslo, L. Jackson, H. Moradi, J. R. Oliver, M. Chowd-\\nhury, T. Vyas, A. Bon Nieves, A. Lawrence, S. F. Wagoner et al.,\\n‚ÄúPredicting therapeutic response to hypoglossal nerve stimulation using\\ndeep learning,‚Äù The Laryngoscope, vol. 134, no. 12, pp. 5210‚Äì5216,\\n2024.\\n[7] T. Zhang, K. Li, X. Chen, C. Zhong, B. Luo, I. Grijalva, B. McCornack,\\nD. Flippo, A. Sharda, and G. Wang, ‚ÄúAphid cluster recognition and\\ndetection in the wild using deep learning models,‚Äù Scientific Reports,\\nvol. 13, no. 1, p. 13410, 2023.\\n[8] R. Rahman, C. Indris, G. Bramesfeld, T. Zhang, K. Li, X. Chen,\\nI. Grijalva, B. McCornack, D. Flippo, A. Sharda et al., ‚ÄúA new dataset\\nand comparative study for aphid cluster detection and segmentation in\\nsorghum fields,‚Äù Journal of Imaging, vol. 10, no. 5, p. 114, 2024.\\n[9] X. Xiao, Q. V. Hu, and G. Wang, ‚ÄúEdge-aware multi-task network\\nfor integrating quantification segmentation and uncertainty prediction of\\nliver tumor on multi-modality non-contrast mri,‚Äù in International Confer-\\nence on Medical Image Computing and Computer-Assisted Intervention.\\nSpringer, 2023, pp. 652‚Äì661.\\n[10] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\\nrecognition,‚Äù in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770‚Äì778.\\n[11] G. Bertasius, H. Wang, and L. Torresani, ‚ÄúIs space-time attention all\\nyou need for video understanding?‚Äù in ICML, vol. 2, no. 3, 2021, p. 4.\\n[12] R. Child, S. Gray, A. Radford, and I. Sutskever, ‚ÄúGenerating long\\nsequences with sparse transformers,‚Äù arXiv preprint arXiv:1904.10509,\\n2019.\\n[13] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin,\\nI. Laptev, N. Neverova, G. Synnaeve, J. Verbeek et al., ‚ÄúXcit: Cross-\\ncovariance image transformers,‚Äù Advances in neural information pro-\\ncessing systems, vol. 34, pp. 20 014‚Äì20 027, 2021.\\n[14] A. Gu and T. Dao, ‚ÄúMamba: Linear-time sequence modeling with\\nselective state spaces,‚Äù arXiv preprint arXiv:2312.00752, 2023.\\n[15] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, ‚ÄúVision\\nmamba: Efficient visual representation learning with bidirectional state\\nspace model,‚Äù arXiv preprint arXiv:2401.09417, 2024.\\n[16] K. Li, X. Li, Y. Wang, Y. He, Y. Wang, L. Wang, and Y. Qiao,\\n‚ÄúVideomamba: State space model for efficient video understanding,‚Äù in\\nEuropean Conference on Computer Vision.\\nSpringer, 2025, pp. 237‚Äì\\n255.\\n[17] H. Qu, L. Ning, R. An, W. Fan, T. Derr, H. Liu, X. Xu, and Q. Li, ‚ÄúA\\nsurvey of mamba,‚Äù arXiv preprint arXiv:2408.01129, 2024.\\n[18] B. N. Patro and V. S. Agneeswaran, ‚ÄúMamba-360: Survey of state space\\nmodels as transformer alternative for long sequence modelling: Methods,\\napplications, and challenges,‚Äù arXiv preprint arXiv:2404.16112, 2024.\\n[19] Y. Yue and Z. Li, ‚ÄúMedmamba: Vision mamba for medical image\\nclassification,‚Äù arXiv preprint arXiv:2403.03849, 2024.\\n[20] S. Bansal, S. Madisetty, M. Z. U. Rehman, C. S. Raghaw, G. Duggal,\\nN. Kumar et al., ‚ÄúA comprehensive survey of mamba architectures for\\nmedical image analysis: Classification, segmentation, restoration and\\nbeyond,‚Äù arXiv preprint arXiv:2410.02362, 2024.\\n[21] R. Xu, S. Yang, Y. Wang, B. Du, and H. Chen, ‚ÄúA survey on\\nvision mamba: Models, applications and challenges,‚Äù arXiv preprint\\narXiv:2404.18861, 2024.\\n[22] H. Zhang, Y. Zhu, D. Wang, L. Zhang, T. Chen, Z. Wang, and Z. Ye,\\n‚ÄúA survey on visual mamba,‚Äù Applied Sciences, vol. 14, no. 13, p. 5683,\\n2024.\\n[23] Z. Zhang and K. T. Chong, ‚ÄúComparison between first-order hold with\\nzero-order hold in discretization of input-delay nonlinear systems,‚Äù in\\n2007 International Conference on Control, Automation and Systems.\\nIEEE, 2007, pp. 2892‚Äì2896.\\n[24] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu,\\n‚ÄúVmamba: Visual state space model,‚Äù arXiv preprint arXiv:2401.10166,\\n2024.\\n[25] F. Wang, J. Wang, S. Ren, G. Wei, J. Mei, W. Shao, Y. Zhou, A. Yuille,\\nand C. Xie, ‚ÄúMamba-r: Vision mamba also needs registers,‚Äù arXiv\\npreprint arXiv:2405.14858, 2024.\\n[26] Y. Shi, M. Dong, M. Li, and C. Xu, ‚ÄúVssd: Vision mamba with non-\\ncausal state space duality,‚Äù arXiv preprint arXiv:2407.18559, 2024.\\n[27] T. Huang, X. Pei, S. You, F. Wang, C. Qian, and C. Xu, ‚ÄúLocalmamba:\\nVisual state space model with windowed selective scan,‚Äù arXiv preprint\\narXiv:2403.09338, 2024.\\n[28] E. Baty, A. H. D¬¥ƒ±az, C. Bridges, R. Davidson, S. Eckersley, and\\nS. Hadfield, ‚ÄúMamba2d: A natively multi-dimensional state-space model\\nfor vision tasks,‚Äù arXiv preprint arXiv:2412.16146, 2024.\\n[29] S. Lee, J. Choi, and H. J. Kim, ‚ÄúEfficientvim: Efficient vision mamba\\nwith hidden state mixer based state space duality,‚Äù arXiv preprint\\narXiv:2411.15241, 2024.\\n[30] T. Dao and A. Gu, ‚ÄúTransformers are ssms: Generalized models and ef-\\nficient algorithms through structured state space duality,‚Äù arXiv preprint\\narXiv:2405.21060, 2024.\\n[31] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. LuÀáci¬¥c, and C. Schmid,\\n‚ÄúVivit: A video vision transformer,‚Äù in Proceedings of the IEEE/CVF\\ninternational conference on computer vision, 2021, pp. 6836‚Äì6846.\\n[32] H. Lu, A. A. Salah, and R. Poppe, ‚ÄúVideomambapro: A leap forward\\nfor mamba in video understanding,‚Äù arXiv preprint arXiv:2406.19006,\\n2024.\\n[33] G. Chen, Y. Huang, J. Xu, B. Pei, Z. Chen, Z. Li, J. Wang, K. Li, T. Lu,\\nand L. Wang, ‚ÄúVideo mamba suite: State space model as a versatile\\nalternative for video understanding,‚Äù arXiv preprint arXiv:2403.09626,\\n2024.\\n[34] Y. Yang, Z. Xing, and L. Zhu, ‚ÄúVivim: a video vision mamba for medical\\nvideo object segmentation,‚Äù arXiv preprint arXiv:2401.14168, 2024.\\n[35] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classification\\nwith deep convolutional neural networks,‚Äù Advances in neural informa-\\ntion processing systems, vol. 25, 2012.\\n[36] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, ‚ÄúObject detection with deep\\nlearning: A review,‚Äù IEEE transactions on neural networks and learning\\nsystems, vol. 30, no. 11, pp. 3212‚Äì3232, 2019.\\n[37] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, ‚ÄúUnified perceptual\\nparsing for scene understanding,‚Äù in Proceedings of the European\\nconference on computer vision (ECCV), 2018, pp. 418‚Äì434.\\n[38] H. Yu, Z. Yang, L. Tan, Y. Wang, W. Sun, M. Sun, and Y. Tang, ‚ÄúMeth-\\nods and datasets on semantic segmentation: A review,‚Äù Neurocomputing,\\nvol. 304, pp. 82‚Äì103, 2018.\\n[39] S. Hao, Y. Zhou, and Y. Guo, ‚ÄúA brief survey on semantic segmentation\\nwith deep learning,‚Äù Neurocomputing, vol. 406, pp. 302‚Äì321, 2020.\\n[40] H. Qin, Y. Fu, J. Chen, M. A. El-Yacoubi, X. Gao, and F. Xi, ‚ÄúNeural\\narchitecture search based global-local vision mamba for palm-vein\\nrecognition,‚Äù arXiv preprint arXiv:2408.05743, 2024.\\n[41] Y. Zou, Y. Chen, Z. Li, L. Zhang, and H. Zhao, ‚ÄúVenturing into uncharted\\nwaters: The navigation compass from transformer to mamba,‚Äù arXiv\\npreprint arXiv:2406.16722, 2024.\\n[42] Z. You, H. Wang, Q. Zhao, and J. Wang, ‚ÄúMambabev: An efficient 3d\\ndetection model with mamba2,‚Äù arXiv preprint arXiv:2410.12673, 2024.\\n[43] C. Yang, Z. Chen, M. Espinosa, L. Ericsson, Z. Wang, J. Liu, and E. J.\\nCrowley, ‚ÄúPlainmamba: Improving non-hierarchical mamba in visual\\nrecognition,‚Äù arXiv preprint arXiv:2403.17695, 2024.\\n[44] C. Xiao, M. Li, Z. Zhang, D. Meng, and L. Zhang, ‚ÄúSpatial-mamba:\\nEffective visual state space models via structure-aware state fusion,‚Äù\\narXiv preprint arXiv:2410.15091, 2024.\\n[45] H. Shen, Z. Wan, X. Wang, and M. Zhang, ‚ÄúFamba-v: Fast vision mamba\\nwith cross-layer token fusion,‚Äù arXiv preprint arXiv:2409.09808, 2024.\\n[46] Y. Ding, H. Qin, Q. Yan, Z. Chai, J. Liu, X. Wei, and X. Liu,\\n‚ÄúTowards accurate post-training quantization for vision transformer,‚Äù in\\nProceedings of the 30th ACM international conference on multimedia,\\n2022, pp. 5380‚Äì5388.\\n[47] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman,\\n‚ÄúToken merging: Your vit but faster,‚Äù arXiv preprint arXiv:2210.09461,\\n2022.\\n[48] Q. Cao, B. Paranjape, and H. Hajishirzi, ‚ÄúPumer: Pruning and merg-\\ning tokens for efficient vision language models,‚Äù arXiv preprint\\narXiv:2305.17530, 2023.\\n9\\n[49] J. Qiao, J. Liao, W. Li, Y. Zhang, Y. Guo, Y. Wen, Z. Qiu, J. Xie,\\nJ. Hu, and S. Lin, ‚ÄúHi-mamba: Hierarchical mamba for efficient image\\nsuper-resolution,‚Äù arXiv preprint arXiv:2410.10140, 2024.\\n[50] A. Gu, K. Goel, and C. R¬¥e, ‚ÄúEfficiently modeling long sequences with\\nstructured state spaces,‚Äù arXiv preprint arXiv:2111.00396, 2021.\\n[51] K. He, G. Gkioxari, P. Doll¬¥ar, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in\\nProceedings of the IEEE international conference on computer vision,\\n2017, pp. 2961‚Äì2969.\\n[52] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\\nA large-scale hierarchical image database,‚Äù in 2009 IEEE conference on\\ncomputer vision and pattern recognition.\\nIeee, 2009, pp. 248‚Äì255.\\n[53] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and\\nA. Torralba, ‚ÄúSemantic understanding of scenes through the ade20k\\ndataset,‚Äù International Journal of Computer Vision, vol. 127, pp. 302‚Äì\\n321, 2019.\\n[54] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\\ncontext,‚Äù in Computer Vision‚ÄìECCV 2014: 13th European Conference,\\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.\\nSpringer, 2014, pp. 740‚Äì755.\\n[55] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-\\nnarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., ‚ÄúThe kinetics\\nhuman action video dataset,‚Äù arXiv preprint arXiv:1705.06950, 2017.\\n[56] J. Carreira and A. Zisserman, ‚ÄúQuo vadis, action recognition? a new\\nmodel and the kinetics dataset,‚Äù in proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017, pp. 6299‚Äì6308.\\n[57] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. West-\\nphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al.,\\n‚ÄúThe‚Äù something something‚Äù video database for learning and evaluat-\\ning visual common sense,‚Äù in Proceedings of the IEEE international\\nconference on computer vision, 2017, pp. 5842‚Äì5850.\\n[58] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,\\n‚ÄúMasked-attention mask transformer for universal image segmentation,‚Äù\\nin Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, 2022, pp. 1290‚Äì1299.\\n[59] M. Lin, G. Xu, Y. Wang, X. Wang, and X. Yang, ‚ÄúFlowmamba: Learning\\npoint cloud scene flow with global motion propagation,‚Äù arXiv preprint\\narXiv:2412.17366, 2024.\\n[60] C. Wang, X. Liu, C. Li, Y. Liu, and Y. Yuan, ‚ÄúPv-ssm: Exploring pure\\nvisual state space model for high-dimensional medical data analysis,‚Äù in\\n2024 IEEE International Conference on Bioinformatics and Biomedicine\\n(BIBM).\\nIEEE, 2024, pp. 2542‚Äì2549.\\n[61] T.-N.-Q. Nguyen, Q.-H. Ho, D.-T. Nguyen, H.-M.-Q. Le, V.-T. Pham,\\nand T.-T. Tran, ‚ÄúMambau-lite: A lightweight model based on mamba\\nand integrated channel-spatial attention for skin lesion segmentation,‚Äù\\narXiv preprint arXiv:2412.01405, 2024.\\n[62] K. Cao, X. He, T. Hu, C. Xie, J. Zhang, M. Zhou, and D. Hong, ‚ÄúShuffle\\nmamba: State space models with random shuffle for multi-modal image\\nfusion,‚Äù arXiv preprint arXiv:2409.01728, 2024.\\n[63] Y. Zhan, Z. Zeng, H. Liu, X. Tan, and Y. Tian, ‚ÄúMambasod: Dual\\nmamba-driven cross-modal fusion network for rgb-d salient object\\ndetection,‚Äù arXiv preprint arXiv:2410.15015, 2024.\\n[64] N. K¬®opr¬®uc¬®u, D. Okpekpe, and A. Orvieto, ‚ÄúNimba: Towards robust\\nand principled processing of point clouds with ssms,‚Äù arXiv preprint\\narXiv:2411.00151, 2024.\\n[65] T. Wang, W. Wen, J. Zhai, K. Xu, and H. Luo, ‚ÄúSerialized point mamba:\\nA serialized point cloud mamba segmentation model,‚Äù arXiv preprint\\narXiv:2407.12319, 2024.\\n[66] Z. Wang, C. Li, H. Xu, and X. Zhu, ‚ÄúMamba yolo: Ssms-based yolo\\nfor object detection,‚Äù arXiv preprint arXiv:2406.05835, 2024.\\n[67] Y. Liu, Y. Ge, M. Li, G. Zheng, B. Sun, and F.-Y. Wang, ‚ÄúPillarmamba: A\\nlightweight mamba-based model for 3d object detection,‚Äù in 2024 IEEE\\n4th International Conference on Digital Twins and Parallel Intelligence\\n(DTPI).\\nIEEE, 2024, pp. 652‚Äì655.\\n[68] T. Ning, K. Lu, X. Jiang, and J. Xue, ‚ÄúMambadetr: Query-based\\ntemporal modeling using state space model for multi-view 3d object\\ndetection,‚Äù arXiv preprint arXiv:2411.13628, 2024.\\n[69] S. T. Inaganti and G. Petrenko, ‚ÄúMambatron: Efficient cross-modal point\\ncloud enhancement using aggregate selective state space modeling,‚Äù\\narXiv preprint arXiv:2501.16384, 2025.\\n[70] A. Sinha, M. S. Raj, P. Wang, A. Helmy, and S. Das, ‚ÄúMs-temba: Multi-\\nscale temporal mamba for efficient temporal action detection,‚Äù arXiv\\npreprint arXiv:2501.06138, 2025.\\n10\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text(\"./PDFs/2502.07161.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Model to interpret and store the figures and table in the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we create a Pydantic schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product schema:\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"type\": {\n",
      "      \"description\": \"Type of element to extract\",\n",
      "      \"enum\": [\n",
      "        \"table\",\n",
      "        \"image\"\n",
      "      ],\n",
      "      \"title\": \"Type\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"page\": {\n",
      "      \"description\": \"Number of the page to extract from\",\n",
      "      \"title\": \"Page\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"number\": {\n",
      "      \"description\": \"Number of the element to extract\",\n",
      "      \"title\": \"Number\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": {\n",
      "      \"description\": \"Description of the element to extract\",\n",
      "      \"title\": \"Description\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"headline\": {\n",
      "      \"default\": null,\n",
      "      \"description\": \"Title of the element to extract\",\n",
      "      \"title\": \"Headline\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"subheadline\": {\n",
      "      \"default\": null,\n",
      "      \"description\": \"Subtitle of the element to extract\",\n",
      "      \"title\": \"Subheadline\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"text\": {\n",
      "      \"default\": null,\n",
      "      \"description\": \"Text of the element to extract\",\n",
      "      \"title\": \"Text\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"type\",\n",
      "    \"page\",\n",
      "    \"number\",\n",
      "    \"description\"\n",
      "  ],\n",
      "  \"title\": \"Extract\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from typing import Literal\n",
    "# Step 1: Define our data contract\n",
    "class Extract(BaseModel):\n",
    "    type: Literal[\"table\", \"image\"] = Field(..., description=\"Type of element to extract\")\n",
    "    page: str = Field(..., description=\"Number of the page to extract from\")\n",
    "    number: str = Field(..., description=\"Number of the element to extract\")\n",
    "    description: str = Field(..., description=\"Description of the element to extract\")\n",
    "    headline: str = Field(None, description=\"Title of the element to extract\")\n",
    "    subheadline: str = Field(None, description=\"Subtitle of the element to extract\")\n",
    "    text: str = Field(None, description=\"Text of the element to extract\")\n",
    "\n",
    "print(\"Product schema:\")\n",
    "print(json.dumps(Extract.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractResponse schema:\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"Extract\": {\n",
      "      \"properties\": {\n",
      "        \"type\": {\n",
      "          \"description\": \"Type of element to extract, a table or a figure\",\n",
      "          \"enum\": [\n",
      "            \"table\",\n",
      "            \"image\"\n",
      "          ],\n",
      "          \"title\": \"Type\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"page\": {\n",
      "          \"description\": \"Number of the page to extract from\",\n",
      "          \"title\": \"Page\",\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"number\": {\n",
      "          \"description\": \"Number of the element to extract\",\n",
      "          \"title\": \"Number\",\n",
      "          \"type\": \"integer\"\n",
      "        },\n",
      "        \"description\": {\n",
      "          \"description\": \"Description of the element to extract\",\n",
      "          \"title\": \"Description\",\n",
      "          \"type\": \"string\"\n",
      "        },\n",
      "        \"headline\": {\n",
      "          \"anyOf\": [\n",
      "            {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"null\"\n",
      "            }\n",
      "          ],\n",
      "          \"default\": null,\n",
      "          \"description\": \"Title of the element to extract\",\n",
      "          \"title\": \"Headline\"\n",
      "        },\n",
      "        \"subheadline\": {\n",
      "          \"anyOf\": [\n",
      "            {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"null\"\n",
      "            }\n",
      "          ],\n",
      "          \"default\": null,\n",
      "          \"description\": \"Subtitle of the element to extract\",\n",
      "          \"title\": \"Subheadline\"\n",
      "        },\n",
      "        \"text\": {\n",
      "          \"anyOf\": [\n",
      "            {\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"null\"\n",
      "            }\n",
      "          ],\n",
      "          \"default\": null,\n",
      "          \"description\": \"Text of the element to extract\",\n",
      "          \"title\": \"Text\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"type\",\n",
      "        \"page\",\n",
      "        \"number\",\n",
      "        \"description\"\n",
      "      ],\n",
      "      \"title\": \"Extract\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"elements\": {\n",
      "      \"description\": \"List of tables or images extracted from the page\",\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/Extract\"\n",
      "      },\n",
      "      \"title\": \"Elements\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"elements\"\n",
      "  ],\n",
      "  \"title\": \"ExtractResponse\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Optional\n",
    "\n",
    "class Extract(BaseModel):\n",
    "    type: Literal[\"table\", \"image\"] = Field(..., description=\"Type of element to extract, a table or a figure\")\n",
    "    page: int = Field(..., description=\"Number of the page to extract from\")\n",
    "    number: int = Field(..., description=\"Number of the element to extract\")\n",
    "    description: str = Field(..., description=\"Description of the element to extract\")\n",
    "    headline: Optional[str] = Field(None, description=\"Title of the element to extract\")\n",
    "    subheadline: Optional[str] = Field(None, description=\"Subtitle of the element to extract\")\n",
    "    text: Optional[str] = Field(None, description=\"Text of the element to extract\")\n",
    "\n",
    "class ExtractResponse(BaseModel):\n",
    "    elements: List[Extract] = Field(..., description=\"List of tables or images extracted from the page\")\n",
    "\n",
    "print(\"ExtractResponse schema:\")\n",
    "print(json.dumps(ExtractResponse.model_json_schema(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements=[Extract(type='table', page=3, number=1, description='This table lists the key innovations and characteristics of the Mamba architecture. It outlines core concepts like its design choices, quantization features, and performance metrics. The table is organized around the fundamental components of Mamba: its key design aspects, its use of implicit convolutions, and its optimization techniques.', headline=None, subheadline=None, text=None), Extract(type='table', page=4, number=1, description=\"This table compares Mamba and existing models (ViM, VideoMamba) based on various metrics like computational cost, model size, and performance on specific tasks. It highlights Mamba's advantages, particularly in terms of computational efficiency and model size compared to Transformers.\", headline=None, subheadline=None, text=None), Extract(type='image', page=1, number=1, description='This figure shows the Mamba architecture, depicting the implicit convolution layers, the gating mechanism, and the overall flow of data through the model. The visual representation clearly illustrates the core building blocks of Mamba, showcasing its unique approach to sequence modeling.', headline=None, subheadline=None, text=None), Extract(type='image', page=2, number=1, description='This figure shows a visualization of the Mamba architecture applied to a semantic segmentation task. The image demonstrates how the model processes image patches and generates segmentation maps, showcasing its versatility in visual tasks.', headline=None, subheadline=None, text=None), Extract(type='image', page=4, number=2, description='This figure is a visualization of the Mamba architecture applied to a semantic segmentation task. The image shows the output produced by the model when processing image patches, and generating segmentation maps.', headline=None, subheadline=None, text=None)]\n"
     ]
    }
   ],
   "source": [
    "# interact with model (locally)\n",
    "response = ollama.chat(\n",
    "    model='gemma3:4b',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Can you find and describe all tables and images on this page?\",\n",
    "        'images': ['/Users/jacopocirica/Desktop/AI Bootcamp/PDFs/images_v1/2502.07161-0.png']\n",
    "    }],\n",
    "    stream=False,\n",
    "    format=ExtractResponse.model_json_schema(),\n",
    ")\n",
    "\n",
    "\n",
    "final = ExtractResponse.model_validate_json(response.message.content)\n",
    "print(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryResponse(BaseModel):\n",
    "    answer: Literal[\"Yes\", \"No\"] = Field(..., description=\"Whether the image contains Tables or Figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model='llava:latest',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Are there Figures (charts or diagram) or tables? Please answer only with 'Yes' or 'No'.\",\n",
    "        'images': ['/Users/jacopocirica/Desktop/AI Bootcamp/PDFs/images_v1/2502.07161-3.png']\n",
    "    }],\n",
    "    stream=False,\n",
    "    format=BinaryResponse.model_json_schema(),\n",
    ")\n",
    "\n",
    "final = BinaryResponse.model_validate_json(response.message.content)\n",
    "print(final.answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableandimage(page: str):\n",
    "    response = ollama.chat(\n",
    "    model='llava:latest',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Are there Figures (charts or diagram) or tables? Please answer only with 'Yes' or 'No'.\",\n",
    "        'images': [page]\n",
    "    }],\n",
    "    stream=False,\n",
    "    format=BinaryResponse.model_json_schema(),\n",
    ")\n",
    "\n",
    "    final = BinaryResponse.model_validate_json(response.message.content)\n",
    "\n",
    "    return final.answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Router with Table/Figure extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page 0 (no table or image)\n",
      "Skipping page 5 (no table or image)\n",
      "Skipping page 6 (no table or image)\n",
      "Skipping page 9 (no table or image)\n",
      "{\n",
      "  \"1\": [\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 1,\n",
      "      \"description\": \"Mamba Block Architecture with Selective State Space Model:  This table shows a diagram of the Mamba architecture, which utilizes a selective state space model. The diagram illustrates the key components including the input, state, and output layers, with emphasis on the selective state space modeling. It is used to show how information is integrated for a more efficient hidden state model.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"Bi-Directional Selective Scanning Mechanism: This table depicts a diagram illustrating the mechanism of the Bi-Directional Selective Scanning Mechanism. It shows the sequential processing flow of information from two preceding and subsequent SSMs, forming the core of the mechanism. \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"3D Spatiotemporal Bidirectional Scanning: This image shows an illustration of the 3D Spatiotemporal Bidirectional Scanning. It combines spatial data with temporal data for more comprehensive analysis.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 1,\n",
      "      \"description\": \"Single Direction Scan: This image illustrates a single-direction scan, where data is processed in a linear fashion to identify potential patterns. It's part of the Bi-Directional Selective Scanning Mechanism.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"Bidirectional Scan: This image shows the bidirectional scanning process, enabling the system to examine data from both past and future frames to enhance temporal understanding.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"Temporal First Scanning: This image depicts the temporal first scanning process, a key element in how data is initially processed in a sequential manner.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 4,\n",
      "      \"description\": \"Information/Spatiotemporal Scanning: This depicts a scanning system that integrates both spatial and temporal data.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 5,\n",
      "      \"description\": \"Vision Mamba (ViM) Block: A block diagram illustrating the core components of the ViM block, emphasizing position sensitivity and other aspects of its design.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 6,\n",
      "      \"description\": \"Capture Global Context: Illustrates how the system leverages its ability to capture global context.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ],\n",
      "  \"2\": [\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 1,\n",
      "      \"description\": \"This figure shows a flowchart for the Vision Simba (ViM) architecture. It includes a convolutional layer, a spatial attention block, a bottleneck layer, and a spatial attention block. This block is used for main processing and uses bidirectional RNNs or Transformers.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"This figure presents the PlainMamba architecture, featuring a bottleneck, a spatial attention block, and a block-coordinate attention block. It is designed for processing visual data using block scanning and sequence modeling.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"This figure illustrates the LocalMamba architecture, comprising a bottleneck, a block-coordinate attention block, and a spatial attention block. It employs a local and global attention mechanism for efficient processing.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 4,\n",
      "      \"description\": \"This figure depicts the Hierarchical Mamba block. It consists of an attention block and a bottleneck layer for processing visual data using block scanning and sequence modeling.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 5,\n",
      "      \"description\": \"This figure shows the SpatialMamba block. The block uses attention with coordinate-based attention and a spatial attention block.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 6,\n",
      "      \"description\": \"This table summarizes the main attributes of the various Mamba architectures: (a) Vision Mamba (ViM), (b) PlainMamba, (c) SpatialMamba, (d) LocalMamba, (e) Hi-Mamba, used to extract high-resolution image features and for high-resolution image restoration.  The table uses abbreviations of each architectures listed in the figures.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ],\n",
      "  \"3\": [\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"Table 2: The Temporal-Spatial and Spatiotemporal Form of Mamba. This table describes the different forms of Mamba, which are the Temporal-Spatial form, the Spatiotemporal form, and the Temporal-Spatial form. Each form has a different number of parameters and a different level of complexity. The Temporal-Spatial form is the most common form and is the one used in the paper. The Spatiotemporal form is more complex and is used for applications where high accuracy is required. The Temporal-Spatial form is the simplest form and is used for applications where accuracy is less important.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"Figure 3: Model Incorporating a Linear-Complexity Form... This figure describes a model incorporating a linear-complexity form. It's a schematic diagram showing the core components of the model: the local SSM, the window scanning, the residual connection, and the global SSM. It highlights the process of window scanning and how the SSMs interact with the window to capture spatiotemporal information.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 4,\n",
      "      \"description\": \"Table 4: The Performance of Mamba. This table presents the performance metrics of Mamba, including accuracy, speed, and memory usage, compared to other models like ViT, Swin, and ConvNeXt. The table demonstrates Mamba's competitive performance, particularly in terms of speed and memory efficiency.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 5,\n",
      "      \"description\": \"Figure 5: Model Incorporating a Linear-Complexity Form... This figure shows the model incorporating a linear-complexity form. It features the local SSM, the window scanning, the residual connection, and the global SSM, emphasizing the process of window scanning and the interaction between SSMs.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ],\n",
      "  \"4\": [\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 1,\n",
      "      \"description\": \"Figure 1: The original Mamba architecture. This figure displays a block diagram of the original Mamba architecture, composed of an attention-based sequential gating mechanism. The blocks are: 1. Sequential Gating Block, 2. State Space Block, and 3. Attention Block.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"Figure 2: Scanning Mechanisms. (a) and (b) are applied on patches, while (a) uses ruster scanning, which loses crucial information that (b) retains from patches located vertically from each other. Local Scan (c) divides an image into local segments for improved local representation. Scans into local segments for improved local representation, and varying directional limits for improved local representation.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"Figure 5: (a) VideoMamba block and (b) VideoMambaPro Architecture. Both simplify scan, (a) uses the same as [15] adapted for 3D data, (b) uses the concept of residual SSM and the adapted patches in (a). \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 1,\n",
      "      \"number\": 6,\n",
      "      \"description\": \"Table 6: Scanning Mechanisms. (a) and (b) are applied on patches, while (a) uses ruster scanning, which loses crucial information that (b) retains from patches located vertically from each other. Local Scan (c) divides an image into local segments for improved local representation. Scans into local segments for improved local representation, and varying directional limits for improved local representation.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ],\n",
      "  \"7\": [\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 3,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"Table III: Comparison of Different Model Results\\n\\nThis table compares the performance of various models for object detection and human action recognition. It includes metrics such as \\\"FLOPs (M)\\\", \\\"Top 1 Acc\\\", and \\\"Top 5 Acc\\\". The models compared are: \\n\\n*   **VideoMatch:** 481, 38.3, 27.6\\n*   **SpatialMatch:** 481, 54.2, 31.8\\n*   **LocalMatch applied on SpatialMatch:** 481, 62.3, 46.4\\n*   **VSSD:** 542, 70.6, 50.1\\n*   **FSM:** 542, 70.6, 50.1\\n\\n\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 4,\n",
      "      \"number\": 4,\n",
      "      \"description\": \"Table IV: Comparison of Different Video Understanding Model Results\\n\\nThis table compares the performance of various models for object detection and human action recognition. It includes metrics such as \\\"FLOPs (M)\\\", \\\"Top 1 Acc\\\", and \\\"Top 5 Acc\\\". The models compared are:\\n\\n*   **Backbone with Mask R-CNN:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n*   **Backbone with Mask R-CNN SD:** 1626, 81.5, 64.2\\n\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"The graph shown illustrates the overall architecture and major components of the YOLO object detection model. Key features include the backbone network (usually a Convolutional Neural Network), the region proposal network, and the detection head. This diagram provides a visual overview of the YOLO architecture and how different modules are combined to perform object detection.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ],\n",
      "  \"8\": [\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 2,\n",
      "      \"description\": \"Table 2: Quantitative comparisons of models' performance on the dataset.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"table\",\n",
      "      \"page\": 3,\n",
      "      \"number\": 3,\n",
      "      \"description\": \"Table 3: Performance of different models on the dataset, showing their average loss and accuracy.\",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 2,\n",
      "      \"number\": 5,\n",
      "      \"description\": \"Figure 5: A sample of text recognized by the visual space model with windowed selective scan. \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 3,\n",
      "      \"number\": 6,\n",
      "      \"description\": \"Figure 6: Visual space model with windowed selective scan, recognizing a sample of text. \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 4,\n",
      "      \"number\": 7,\n",
      "      \"description\": \"Figure 7: A sample of text recognized by the visual space model with windowed selective scan. \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"image\",\n",
      "      \"page\": 4,\n",
      "      \"number\": 8,\n",
      "      \"description\": \"Figure 8: Visual space model with windowed selective scan, recognizing a sample of text. \",\n",
      "      \"headline\": null,\n",
      "      \"subheadline\": null,\n",
      "      \"text\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "image_dir = \"/Users/jacopocirica/Desktop/AI Bootcamp/PDFs/images_v1\"\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "\n",
    "for img_file in image_files:\n",
    "    page_number = os.path.splitext(img_file)[0].split(\"-\")[-1]  # get page number from filename\n",
    "    image_path = os.path.join(image_dir, img_file)\n",
    "\n",
    "    if tableandimage(image_path) != \"Yes\":\n",
    "        print(f\"Skipping page {page_number} (no table or image)\")\n",
    "        continue  # Skip this page if there's nothing to extract\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='gemma3:4b',\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': \"Can you find and describe all tables and images on this page?\",\n",
    "                'images': [image_path]\n",
    "            }],\n",
    "            stream=False,\n",
    "            format=ExtractResponse.model_json_schema(),\n",
    "        )\n",
    "\n",
    "        parsed = ExtractResponse.model_validate_json(response.message.content)\n",
    "        results[page_number] = [e.model_dump() for e in parsed.elements]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on page {page_number}: {e}\")\n",
    "        results[page_number] = []\n",
    "\n",
    "# --- Final output ---\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put all toghether "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: User query is converted into a keyword search and the pdf is downloaded\n",
    "#### 1.1 function to get the url from the keyword\n",
    "def get_url(topic: str):\n",
    "    tavily_client = TavilyClient(api_key=\"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\")\n",
    "    response = tavily_client.search(query=f\"Tell me the main paper about this topic:{topic}\",include_domains=[\"arxiv.org/abs/\"], search_depth= \"advanced\")\n",
    "    return response['results'][0]['url']\n",
    "#### 1.2 utility function\n",
    "def extract_arxiv_id(url: str) -> str:\n",
    "    return url.rstrip(\"/\").split(\"/\")[-1] + \".pdf\"\n",
    "\n",
    "#### 1.3 function to download the pdf\n",
    "\n",
    "def download_arxiv_pdf(arxiv_url: str, destination_folder: str, filename: str = None):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from an arXiv URL and saves it to a specified folder.\n",
    "\n",
    "    Args:\n",
    "        arxiv_url (str): Direct link to the arXiv PDF (e.g., https://arxiv.org/pdf/2311.17633).\n",
    "        destination_folder (str): Local folder path to save the file.\n",
    "        filename (str, optional): Name to save the file as (e.g., 'paper.pdf').\n",
    "                                  If None, it uses the arXiv ID as the filename.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the saved file.\n",
    "    \"\"\"\n",
    "    # Extract default filename from URL if not provided\n",
    "    if filename is None:\n",
    "        filename = arxiv_url.strip(\"/\").split(\"/\")[-1] + \".pdf\"\n",
    "\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(arxiv_url)\n",
    "        response.raise_for_status()  # Raise error for bad responses\n",
    "        with open(destination_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"‚úÖ PDF downloaded to: {destination_path}\")\n",
    "        return destination_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "        return None\n",
    "def convert_to_pdf_url(url: str) -> str:\n",
    "    return url.replace(\"abs\", \"pdf\").replace(\"html\", \"pdf\")\n",
    "   \n",
    "def identify_keywords_download_pdf(user_query: str):\n",
    "    available_functions: Dict[str, Callable] = {\n",
    "    \"get_url\": get_url}\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='qwen3:4b',\n",
    "        messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f\"Can you tell me the main paper about this topic: {user_query}\",\n",
    "        }],\n",
    "        tools=[get_url])\n",
    "    if response.message.tool_calls:\n",
    "        for tool_call in response.message.tool_calls:\n",
    "            if tool_call.function.name == \"get_url\":\n",
    "                print(\"Topic: \", tool_call.function.arguments['topic'])\n",
    "                print(\"Tool call: \", tool_call.function.name)\n",
    "                url=available_functions[tool_call.function.name](tool_call.function.arguments['topic'])\n",
    "                url_pdf=convert_to_pdf_url(url)\n",
    "                print(\"Downloading URL: \", url_pdf)\n",
    "                download_arxiv_pdf(url_pdf, \"./PDFs\")\n",
    "\n",
    "                return extract_arxiv_id(url_pdf)\n",
    "    else:\n",
    "        print(\"No URL found\")\n",
    "        return None\n",
    "             \n",
    "### Step 2 convert each PDFs to image, extract images and text \n",
    "\n",
    "#### Step 2.1 Extract text\n",
    "def extract_text(pdf_path):\n",
    "    pdf=fitz.open(pdf_path)\n",
    "    text=\"\".join([page.get_text() for page in pdf])\n",
    "\n",
    "    return text\n",
    "\n",
    "####Step 2.2 convert each page to image\n",
    "\n",
    "\n",
    "# pdf_path: the folder of all the PDF files\n",
    "# saved_path: the path of the saved page images\n",
    "def convert_pdf_to_image(pdf_path, pdf_file, saved_path):\n",
    "\n",
    "    if not os.path.exists(saved_path):\n",
    "        os.mkdir(saved_path)\n",
    "    else:\n",
    "        files = glob.glob('saved_path/*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "    try:\n",
    "        fitz.TOOLS.mupdf_warnings()  # empty the problem message container\n",
    "        doc = fitz.open(pdf_path + \"/\" + pdf_file)\n",
    "        warnings = fitz.TOOLS.mupdf_warnings()\n",
    "        if warnings:\n",
    "            print(warnings)\n",
    "            raise RuntimeError()\n",
    "\n",
    "        for page in doc:  # iterate through the pages\n",
    "            pix = page.get_pixmap()  # render page to an image\n",
    "            pix.save(saved_path + \"/\" + f\"{pdf_file[:-4]}-{page.number}.png\")  # store image as a PNG\n",
    "        return\n",
    "\n",
    "    except:\n",
    "        print(\"error when opening the pdf file {}\".format(pdf_file))\n",
    "        return None\n",
    "\n",
    "\n",
    "#Step 2.3 Decide if the image contains figures or tables\n",
    "def tableandimage(page: str):\n",
    "    response = ollama.chat(\n",
    "    model='llava:latest',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': \"Are there Figures (charts or diagram) or tables? Please answer only with 'Yes' or 'No'.\",\n",
    "        'images': [page]\n",
    "    }],\n",
    "    stream=False,\n",
    "    format=BinaryResponse.model_json_schema(),\n",
    ")\n",
    "\n",
    "    final = BinaryResponse.model_validate_json(response.message.content)\n",
    "\n",
    "    return final.answer\n",
    "\n",
    "#Step 2.4 Extract the description of each table or image\n",
    "def extract_tableandimage_content(image_dir: str):\n",
    "    results={}\n",
    "    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "    for img_file in image_files:\n",
    "        page_number = os.path.splitext(img_file)[0].split(\"-\")[-1]  # get page number from filename\n",
    "        image_path = os.path.join(image_dir, img_file)\n",
    "\n",
    "        if tableandimage(image_path) != \"Yes\":\n",
    "            print(f\"Skipping page {page_number} (no table or image)\")\n",
    "            continue  # Skip this page if there's nothing to extract\n",
    "\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "            model='gemma3:4b',\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': \"Can you find and describe all tables and images on this page?\",\n",
    "                'images': [image_path]\n",
    "            }],\n",
    "            stream=False,\n",
    "            format=ExtractResponse.model_json_schema(),\n",
    "        )\n",
    "\n",
    "            parsed = ExtractResponse.model_validate_json(response.message.content)\n",
    "            results[page_number] = [e.model_dump() for e in parsed.elements]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on page {page_number}: {e}\")\n",
    "            results[page_number] = []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "####Step 2.5 Put togheter PDF text and PDF figures and tables\n",
    "def extract_combine_images_text(user_query, pdf_path, saved_path):\n",
    "    pdf_file=identify_keywords_download_pdf(user_query)\n",
    "    file_path=pdf_path+\"/\"+pdf_file\n",
    "    text_pdf=extract_text(file_path)\n",
    "    convert_pdf_to_image(pdf_path, pdf_file, saved_path)\n",
    "    figures_image_content=extract_tableandimage_content(saved_path)\n",
    "    figures_image_content= str(figures_image_content)\n",
    "    content_pdf = f\"PDF content: {text_pdf}. DESCRITPION FIGURES AND TABLES: {figures_image_content}\"\n",
    "    \n",
    "\n",
    "    return content_pdf\n",
    "\n",
    "###Step 3: Conversate with the PDF\n",
    "#### Step 3.1 Edit the response\n",
    "def remove_think_block(response_str):\n",
    "    return re.sub(r'<think>.*?</think>', '', response_str, flags=re.DOTALL).strip()\n",
    "    \n",
    "\n",
    "def reply_user(query: str, content_pdf: str)->str:\n",
    "    system_prompt=f\"\"\"# Role and Objective\n",
    "\n",
    "You are a research assistant. Your task is to read technical articles and reply user questions. You will have the PDF content\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Review the article provided by the user and generate a response to user query\n",
    "\n",
    "# Guidelines\n",
    "\n",
    "- Write in a neutral and academic tone.\n",
    "- Use simple, precise language to ensure clarity for a broad audience.\n",
    "- Keep the responses concise (150-300 words) unless otherwise specified.\n",
    "- Assume the audience has general technical knowledge but may not be familiar with the specific field of the paper.\n",
    "- Please limit bullets under key terms, key findings, and further reading, sections to the 3-5 most essential.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt=f\"\"\"\n",
    "Given the content of an academic paper, reply the the user query\n",
    "#PDF Content:\n",
    "{content_pdf}\n",
    "#User query:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = ollama.chat(\n",
    "    model='qwen3:4b',\n",
    "    messages=[{\n",
    "        'role': 'system',\n",
    "        'content': system_prompt\n",
    "    },{\n",
    "        'role': 'user',\n",
    "        'content': user_prompt\n",
    "    }])\n",
    "    final_response= remove_think_block(response.message.content)\n",
    "    return final_response\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  Mamba Architecture\n",
      "Tool call:  get_url\n",
      "Downloading URL:  https://arxiv.org/pdf/2502.07161\n",
      "‚úÖ PDF downloaded to: ./PDFs/2502.07161.pdf\n",
      "Skipping page 3 (no table or image)\n",
      "Skipping page 5 (no table or image)\n",
      "Skipping page 8 (no table or image)\n",
      "Skipping page 9 (no table or image)\n",
      "Content loaded. You can now ask anything about the paper. Type 'exit' to end the chat.\n",
      "\n",
      "\n",
      "Answer:\n",
      "The main topic of the paper is **the comparison and evaluation of various computer vision models** (including MAMBA, VideoMamba, YOLO, Transformers, and others) across different tasks such as **image classification, semantic segmentation, object detection, and video understanding**. The paper analyzes their performance metrics (e.g., FLOPs, accuracy, computational efficiency) to determine their suitability for specific applications, such as resource-constrained environments versus complex scenes. It highlights the trade-offs between computational efficiency, accuracy, and scalability of these models.\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Answer:\n",
      "The first table mentioned in the document is **Table 1** (page 1, number 1), which compares different image classification models. It includes the following:\n",
      "\n",
      "- **Models compared**: VGG, ResNet, and VGG applied to VGGs.\n",
      "- **Dataset**: 400 images.\n",
      "- **Metrics evaluated**: \n",
      "  - **M** (likely model size or parameters).\n",
      "  - **FLOPs** (floating-point operations per second, a measure of computational complexity).\n",
      "  - **ACC** (accuracy, a measure of model performance).\n",
      "\n",
      "The table provides the **model name**, **parameters (M)**, **FLOPs**, and **Accuracy (ACC)** for each model, allowing a direct comparison of their efficiency and performance.\n",
      "\n",
      "---\n",
      "\n",
      "Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chat_about_paper():\n",
    "    # Step 1: Get the keyword and extract the PDF content\n",
    "    keyword = input(\"Enter the paper that you want to look up: \")\n",
    "    content = extract_combine_images_text(keyword, \"./PDFs\", \"./PDFs/images_v2\")\n",
    "    print(\"Content loaded. You can now ask anything about the paper. Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "    # Step 2: Loop for querying the paper content\n",
    "    while True:\n",
    "        query = input(\"Your question: \")\n",
    "        if query.lower().strip() == \"exit\":\n",
    "            print(\"Exiting chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = reply_user(query, content)\n",
    "        \n",
    "        # Optional: clean output to remove <think> block\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(extract_final_answer(response))  # using the function from previous message\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Now you can call this function to start the interactive session\n",
    "chat_about_paper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
